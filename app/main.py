import os
import json
import yaml
import httpx
import logging
import docker
import asyncio
import re
from datetime import datetime
from urllib.parse import quote, unquote, urlparse
from typing import List, Dict, Any, Optional
from collections import Counter
import pytz

# æ£€æŸ¥ä¾èµ–
try:
    import aiofiles
except ImportError:
    print("CRITICAL ERROR: ç¼ºå°‘ 'aiofiles' åº“ã€‚è¯·ç¡®ä¿ requirements.txt ä¸­åŒ…å« aiofiles å¹¶é‡æ–°æ„å»ºé•œåƒï¼")
    exit(1)

from fastapi import FastAPI, HTTPException, UploadFile, File, Request, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# [Scheduler]
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# --- é…ç½®è·¯å¾„ ---
DATA_PATH = "/data"
CONFIG_JSON = os.path.join(DATA_PATH, "config.json")
OUTPUT_YAML = os.path.join(DATA_PATH, "config.yaml")
LOG_FILE = os.path.join(DATA_PATH, "app.log")
DEFAULT_BACKEND = "https://api.v1.mk/sub?target=clash&url="

# --- åˆå§‹åŒ–æ—¥å¿— ---
logger = logging.getLogger("ClashWeb")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)

if not os.path.exists(DATA_PATH):
    try:
        os.makedirs(DATA_PATH)
    except:
        pass
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# --- åˆå§‹åŒ–è°ƒåº¦å™¨ ---
tz = pytz.timezone('Asia/Shanghai')
scheduler = AsyncIOScheduler(timezone=tz)

app = FastAPI(title="ClashWeb")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- æ•°æ®æ¨¡å‹ ---

class UserInfo(BaseModel):
    name: str = "" 
    webUrl: str = "" # [æ–°å¢] æœºåœºå®˜ç½‘åœ°å€
    upload: int = 0
    download: int = 0
    total: int = 0
    expire: int = 0
    update_time: str = ""

class SubHistoryItem(BaseModel):
    url: str
    date: str
    name: Optional[str] = "æœªçŸ¥æœºåœº"
    info: Optional[Dict[str, Any]] = {}
    remarks: Optional[str] = ""

class ConfigModel(BaseModel):
    sub_backend: Optional[str] = ""
    sub_url: Optional[str] = ""
    restart_containers: Optional[str] = "" 
    auto_update: Optional[bool] = False
    cron_expression: Optional[str] = "0 4 * * *" 
    user_info: Optional[UserInfo] = UserInfo()
    sub_history: Optional[List[SubHistoryItem]] = []
    add_groups: Optional[List[Dict[str, Any]]] = []
    del_groups: Optional[List[str]] = []
    add_rules: Optional[List[str]] = []
    del_rules: Optional[List[str]] = []

class DownloadRequest(BaseModel):
    url: str

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def init_data():
    if not os.path.exists(DATA_PATH):
        try:
            os.makedirs(DATA_PATH)
        except: pass
    
    if not os.path.exists(CONFIG_JSON) or os.path.isdir(CONFIG_JSON):
        with open(CONFIG_JSON, 'w') as f: json.dump(ConfigModel().dict(), f)

    if not os.path.exists(OUTPUT_YAML) or os.path.isdir(OUTPUT_YAML):
        with open(OUTPUT_YAML, 'w') as f: f.write("")

def refresh_scheduler():
    try:
        with open(CONFIG_JSON, 'r') as f:
            data = json.load(f)
        
        job_id = 'auto_update_job'
        if scheduler.get_job(job_id):
            scheduler.remove_job(job_id)

        if data.get('auto_update') and data.get('cron_expression'):
            cron_str = data['cron_expression']
            try:
                trigger = CronTrigger.from_crontab(cron_str, timezone=tz)
                scheduler.add_job(scheduled_update_task, trigger, id=job_id, replace_existing=True)
                logger.info(f"âœ… å®šæ—¶ä»»åŠ¡å·²è®¾ç½®: [{cron_str}]")
            except Exception as e:
                logger.error(f"Invalid cron expression: {e}")
        else:
            logger.info("â›”ï¸ å®šæ—¶ä»»åŠ¡å·²å…³é—­")
    except Exception as e:
        logger.error(f"Scheduler refresh failed: {e}")
async def scheduled_update_task():
    logger.info(">>> â³ å¼€å§‹æ‰§è¡Œå®šæ—¶æ›´æ–°ä»»åŠ¡ <<<")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
        
        url = data.get('sub_url')
        if not url:
            logger.warning("æœªé…ç½®è®¢é˜…é“¾æ¥ï¼Œè·³è¿‡æ›´æ–°")
            return

        # æ‰§è¡Œæ›´æ–°é€»è¾‘
        await internal_process_subscription(url, data)
        
        # ä¿å­˜ user_info æ›´æ–°
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(data, indent=2))

        logger.info("âœ… å®šæ—¶æ›´æ–°ä»»åŠ¡å®Œæˆ")

        # [ä¿®å¤] å…¼å®¹ä¸­æ–‡é€—å·
        container_str = data.get('restart_containers', '').replace('ï¼Œ', ',')
        if container_str:
            try:
                client = docker.from_env()
                targets = [name.strip() for name in container_str.split(',') if name.strip()]
                for name in targets:
                    try:
                        client.containers.get(name).restart()
                        logger.info(f"âœ… å®¹å™¨å·²é‡å¯: {name}")
                    except Exception as e:
                        logger.error(f"âŒ é‡å¯å®¹å™¨ {name} å¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"Docker è¿æ¥å¤±è´¥: {e}")
                    
    except Exception as e:
        logger.error(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")

# --- é€»è¾‘åˆ†ç¦»ï¼šä»»åŠ¡1 è·å–åŸå§‹æµé‡ä¿¡æ¯å’Œæœºåœºåç§° ---
async def fetch_original_userinfo(url: str) -> Optional[dict]:
    """ç›´æ¥è¯·æ±‚åŸå§‹è®¢é˜…é“¾æ¥ï¼Œæå– Header ä¸­çš„æµé‡ä¿¡æ¯ã€profile-title å’Œå®˜ç½‘åœ°å€"""
    logger.info(f"ğŸ“¡ [ä¿¡æ¯è·å–] æ­£åœ¨è¯·æ±‚åŸå§‹é“¾æ¥: {url}")
    headers = {"User-Agent": "ClashForAndroid/2.5.12"} 
    
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            # ä½¿ç”¨ GET ä½†é€šè¿‡ stream ç«‹å³å…³é—­ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶
            async with client.stream("GET", url, headers=headers, timeout=30.0) as resp:
                
                # 1. æå–æµé‡ä¿¡æ¯
                user_info_header = None
                for k, v in resp.headers.items():
                    if k.lower() == 'subscription-userinfo':
                        user_info_header = v
                        break
                
                info = {}
                if user_info_header:
                    parts = user_info_header.split(';')
                    for part in parts:
                        if '=' in part:
                            kv = part.strip().split('=')
                            if len(kv) >= 2:
                                info[kv[0].strip()] = int(kv[1].strip())

                # 2. æå–æœºåœºåç§°
                airport_name = ""
                # A. ä¼˜å…ˆæ£€æŸ¥ profile-title
                for k, v in resp.headers.items():
                    if k.lower() == 'profile-title':
                        try: airport_name = unquote(v)
                        except: airport_name = v
                        break
                
                # B. Content-Disposition æå–
                if not airport_name:
                    for k, v in resp.headers.items():
                        if k.lower() == 'content-disposition':
                            m = re.search(r'filename\*?=(?:UTF-8\'\')?([^;]+)', v, re.IGNORECASE)
                            if m:
                                raw_name = m.group(1).strip('"\'')
                                try:
                                    airport_name = unquote(raw_name)
                                    if '.' in airport_name: airport_name = airport_name.rsplit('.', 1)[0]
                                except: pass
                            break
                
                # C. åŸŸåå…œåº•
                if not airport_name:
                    try: airport_name = urlparse(url).netloc
                    except: airport_name = "æœªçŸ¥è®¢é˜…"

                # 3. [æ–°å¢] æå–å®˜ç½‘åœ°å€ (webUrl)
                web_url = ""
                # A. å°è¯•ä»å“åº”å¤´è·å– (Clash æ ‡å‡†å¤´)
                for k, v in resp.headers.items():
                    if k.lower() == 'profile-web-page-url':
                        web_url = v.strip()
                        break
                
                # B. åŸŸåå…œåº•ï¼šå¦‚æœå¤´ä¿¡æ¯æ²¡æœ‰ï¼Œä½¿ç”¨ subscription url çš„ root domain
                if not web_url:
                    try:
                        parsed = urlparse(url)
                        # ç®€å•çš„å‡è®¾ï¼šè®¢é˜…åŸŸåçš„æ ¹é€šå¸¸æ˜¯å®˜ç½‘
                        web_url = f"{parsed.scheme}://{parsed.netloc}"
                    except: pass

                result = {
                    "name": airport_name,
                    "webUrl": web_url, # [æ–°å¢]
                    "upload": info.get("upload", 0),
                    "download": info.get("download", 0),
                    "total": info.get("total", 0),
                    "expire": info.get("expire", 0),
                    "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                logger.info(f"âœ… [ä¿¡æ¯è·å–] æˆåŠŸ: {result}")
                return result

    except Exception as e:
        logger.warning(f"âŒ [ä¿¡æ¯è·å–] è¯·æ±‚å¤±è´¥: {e}")
        return None

# --- é€»è¾‘åˆ†ç¦»ï¼šä»»åŠ¡2 ä¸‹è½½å¹¶è½¬æ¢é…ç½® ---
async def download_and_convert_config(url: str, data: dict) -> bool:
    """è¯·æ±‚è½¬æ¢åç«¯ï¼Œä¸‹è½½ YAMLï¼Œå¹¶åº”ç”¨ Patch"""
    base_url = data.get('sub_backend') or DEFAULT_BACKEND
    if "target=" not in base_url:
        if not base_url.endswith("/"): base_url += "/"
        base_url += "sub?target=clash&url="
    
    encoded_sub_url = quote(url, safe='') 
    full_url = f"{base_url}{encoded_sub_url}"
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    logger.info(f"â¬‡ï¸ [ä¸‹è½½ä»»åŠ¡] æ­£åœ¨è¯·æ±‚è½¬æ¢åç«¯: {full_url}")
    
    config_yaml = ""
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            resp = await client.get(full_url, headers=headers, timeout=60.0)
            if resp.status_code != 200:
                raise Exception(f"è½¬æ¢åç«¯è¿”å›é”™è¯¯ç : {resp.status_code}")
            
            config_yaml = resp.content.decode('utf-8', errors='ignore')
            if "No nodes were found" in config_yaml:
                raise Exception("åç«¯è¿”å› 'No nodes were found'ï¼Œè¯·æ£€æŸ¥è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] ä¸‹è½½å¤±è´¥: {e}")
        raise e

    # è§£æ YAML
    try:
        config = yaml.safe_load(config_yaml)
        if not isinstance(config, dict):
            raise Exception("è§£æç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] YAML è§£æå¤±è´¥: {e}")
        raise Exception("YAML è§£æå¤±è´¥ï¼Œå†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ Clash é…ç½®")

    # åº”ç”¨è¡¥ä¸ (Patch)
    try:
        final_config = apply_patch(config, data)
        output_str = yaml.dump(final_config, allow_unicode=True, sort_keys=False, default_flow_style=False, width=float("inf"))
        yaml.safe_load(output_str) # æ ¡éªŒ
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] é…ç½®å¤„ç†æˆ–æ ¡éªŒå¤±è´¥: {e}")
        raise Exception(f"é…ç½®å¤„ç†å¤±è´¥: {e}")

    # å†™å…¥æ–‡ä»¶
    async with aiofiles.open(OUTPUT_YAML, 'w', encoding='utf-8') as f:
        await f.write(output_str)
    
    logger.info("âœ… [ä¸‹è½½ä»»åŠ¡] é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ config.yaml")
    return True

# --- ä¸»æµç¨‹ ---
async def internal_process_subscription(url: str, data: dict) -> Optional[dict]:
    """
    å¹¶å‘æ‰§è¡Œï¼š
    1. ä»åŸå§‹é“¾æ¥è·å–æµé‡ä¿¡æ¯å’Œåç§°
    2. ä»è½¬æ¢åç«¯è·å–é…ç½®æ–‡ä»¶
    
    è¿”å›ï¼šfetch_original_userinfo çš„ç»“æœ (å¯èƒ½ä¸º None)
    """
    
    task_traffic = fetch_original_userinfo(url)
    task_download = download_and_convert_config(url, data)
    
    results = await asyncio.gather(task_traffic, task_download, return_exceptions=True)
    
    fetched_user_info = results[0]
    download_result = results[1]
    
    # å¤„ç†æµé‡ä¿¡æ¯ç»“æœ
    if isinstance(fetched_user_info, dict):
        data['user_info'] = fetched_user_info
        # æ³¨æ„ï¼šè¿™é‡Œåªæ›´æ–°å†…å­˜ä¸­çš„ data å¯¹è±¡ï¼Œè°ƒç”¨è€…è´Ÿè´£å†™å…¥ config.json
    elif isinstance(fetched_user_info, Exception):
        logger.warning(f"æµé‡ä¿¡æ¯è·å–ä»»åŠ¡å¼‚å¸¸: {fetched_user_info}")

    # å¤„ç†ä¸‹è½½ç»“æœ
    if isinstance(download_result, Exception):
        raise download_result

    return fetched_user_info if isinstance(fetched_user_info, dict) else None

def get_rule_target(rule_str: str) -> str:
    try:
        clean = rule_str.split('#')[0].strip()
        parts = clean.split(',')
        if len(parts) >= 3:
            return parts[2].strip()
    except: pass
    return ""

def clean_rule_for_clash(rule_str: str) -> str:
    return rule_str.split('#')[0].strip()

def apply_patch(config: dict, patch: dict) -> dict:
    config['allow-lan'] = True
    config['external-controller'] = '0.0.0.0:9090'
    if 'bind-address' in config: config['bind-address'] = '*'

    reference_proxies = ["DIRECT", "REJECT"]
    source_groups = config.get('proxy-groups', [])
    for g in source_groups:
        if g.get('type') == 'select' and len(g.get('proxies', [])) > 3:
            reference_proxies = g['proxies']
            break

    del_groups_list = patch.get('del_groups') or []
    add_rules_raw = patch.get('add_rules') or []

    if del_groups_list:
        config['proxy-groups'] = [g for g in config.get('proxy-groups', []) if g['name'] not in del_groups_list]
        new_base_rules = []
        for rule in config.get('rules', []):
            if get_rule_target(rule) not in del_groups_list:
                new_base_rules.append(rule)
        config['rules'] = new_base_rules
        
        valid_add_rules = []
        for rule in add_rules_raw:
            if get_rule_target(rule) not in del_groups_list:
                valid_add_rules.append(rule)
        add_rules_raw = valid_add_rules

    add_groups = patch.get('add_groups') or []
    if add_groups:
        existing_names = {g['name'] for g in config.get('proxy-groups', [])}
        for g in reversed(add_groups):
            if g.get('name') and g['name'] not in existing_names:
                new_group = g.copy()
                current_proxies = new_group.get('proxies', [])
                if not current_proxies or current_proxies == ["DIRECT", "REJECT"]:
                     new_group['proxies'] = list(reference_proxies)
                config.setdefault('proxy-groups', []).insert(0, new_group)

    del_keywords = patch.get('del_rules') or []
    if del_keywords:
        final_rules = []
        for rule in config.get('rules', []):
            clean_rule = clean_rule_for_clash(rule)
            if not any(k in clean_rule for k in del_keywords): 
                final_rules.append(rule)
        config['rules'] = final_rules

    if add_rules_raw:
        for r in reversed(add_rules_raw): 
            clean_r = clean_rule_for_clash(r)
            if clean_r:
                config.setdefault('rules', []).insert(0, clean_r)
             
    return config
@app.on_event("startup")
async def startup_event():
    init_data()
    scheduler.start()
    refresh_scheduler()
    logger.info("Application started, scheduler running.")

@app.get("/api/logs")
async def get_logs(lines: int = 100):
    if not os.path.exists(LOG_FILE):
        return {"logs": []}
    try:
        async with aiofiles.open(LOG_FILE, 'r', encoding='utf-8') as f:
            content = await f.read()
            all_lines = content.splitlines()
            return {"logs": all_lines[-lines:]}
    except Exception as e:
        return {"logs": [f"Error reading logs: {str(e)}"]}

@app.get("/api/data")
async def get_data():
    try:
        if os.path.exists(CONFIG_JSON) and os.path.getsize(CONFIG_JSON) > 0:
            async with aiofiles.open(CONFIG_JSON, 'r') as f:
                content = await f.read()
                data = json.loads(content)
                # ç¡®ä¿ user_info ç»“æ„å®Œæ•´
                if 'user_info' not in data:
                    data['user_info'] = UserInfo().dict()
                else:
                    # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„å­—æ®µ (å¦‚ webUrl)
                    default_info = UserInfo().dict()
                    for k, v in default_info.items():
                        if k not in data['user_info']:
                            data['user_info'][k] = v
                return data
        return {}
    except: return {}

@app.post("/api/data")
async def save_data(data: ConfigModel):
    try:
        payload = data.dict(exclude_none=True)
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(payload, indent=2))
        
        refresh_scheduler()
        return {"status": "success"}
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/backup")
async def backup_config(include_sub: bool = False):
    if not os.path.exists(CONFIG_JSON): raise HTTPException(status_code=404, detail="No config found")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
            
        if not include_sub:
            data['sub_url'] = ""
            data['sub_history'] = []
            
        temp_path = "/tmp/clashweb_backup.json"
        async with aiofiles.open(temp_path, 'w') as f:
            await f.write(json.dumps(data, indent=2))
            
        return FileResponse(temp_path, filename="clashweb_backup.json", media_type="application/json")
    except Exception as e:
        raise HTTPException(500, detail=str(e))

@app.post("/api/restore")
async def restore_config(file: UploadFile = File(...), restore_sub: bool = Form(False)):
    try:
        content = await file.read()
        backup_data = json.loads(content)
        if not isinstance(backup_data, dict): raise ValueError("Format Error")
        
        final_data = backup_data
        if not restore_sub:
            current_data = {}
            if os.path.exists(CONFIG_JSON):
                with open(CONFIG_JSON, 'r') as f: current_data = json.load(f)
            final_data['sub_url'] = current_data.get('sub_url', '')
            final_data['sub_history'] = current_data.get('sub_history', [])
        
        if restore_sub and not final_data.get('sub_url'):
             raise ValueError("å¤‡ä»½æ–‡ä»¶ä¸­æœªåŒ…å«è®¢é˜…ä¿¡æ¯")

        async with aiofiles.open(CONFIG_JSON, "w") as f:
            await f.write(json.dumps(final_data, indent=2))
        
        refresh_scheduler()
        
        summary = {
            "groups": len(final_data.get('add_groups', [])),
            "rules": len(final_data.get('add_rules', [])),
            "sub_status": "å·²è¦†ç›–" if restore_sub else "æœªå˜æ›´",
            "has_sub": bool(final_data.get('sub_url'))
        }
        return {"status": "success", "summary": summary}
    except Exception as e:
        logger.error(f"è¿˜åŸå¤±è´¥: {e}")
        raise HTTPException(status_code=400, detail=f"Restore Failed: {str(e)}")

@app.post("/api/restart_containers")
async def restart_containers():
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
        
        # [ä¿®å¤] å…¼å®¹ä¸­æ–‡é€—å·
        container_str = data.get('restart_containers', '').replace('ï¼Œ', ',')
        targets = [n.strip() for n in container_str.split(',') if n.strip()]

        if not targets: raise HTTPException(400, detail="æœªè®¾ç½®å®¹å™¨")
        
        client = docker.from_env()
        restarted = []
        for name in targets:
            try:
                client.containers.get(name).restart()
                restarted.append(name)
                logger.info(f"æ‰‹åŠ¨è§¦å‘ - å®¹å™¨å·²é‡å¯: {name}")
            except Exception as e:
                logger.error(f"æ‰‹åŠ¨è§¦å‘ - é‡å¯å¤±è´¥ {name}: {e}")
            
        if not restarted:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æœ‰æ•ˆå®¹å™¨")
            
        return {"status": "success", "msg": f"å·²é‡å¯: {', '.join(restarted)}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Docker Error: {str(e)}")

@app.post("/api/download")
async def download_config(req: DownloadRequest):
    if not req.url: raise HTTPException(status_code=400, detail="Missing URL")

    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
    except: data = {}
    
    # ä¸´æ—¶æ›´æ–° URL ä»¥ä¾› process é€»è¾‘ä½¿ç”¨
    data['sub_url'] = req.url
    
    # å°è¯•å¤„ç†è®¢é˜… (ä¸‹è½½ + è·å–ä¿¡æ¯)
    try:
        # è·å–æœ€æ–°ä¿¡æ¯ (fetched_info æ˜¯ fetch_original_userinfo çš„è¿”å›å€¼)
        fetched_info = await internal_process_subscription(req.url, data)
        
        # --- æ ¸å¿ƒæ›´æ–°é€»è¾‘ï¼šæ›´æ–°å†å²è®°å½• ---
        airport_name = "æœªçŸ¥æœºåœº"
        traffic_info = {}
        
        if fetched_info:
            airport_name = fetched_info.get("name", "æœªçŸ¥æœºåœº")
            traffic_info = {
                "upload": fetched_info.get("upload", 0),
                "download": fetched_info.get("download", 0),
                "total": fetched_info.get("total", 0),
                "expire": fetched_info.get("expire", 0)
            }
        
        # 2. æ›´æ–° history
        history = data.get('sub_history', [])
        # ç§»é™¤å·²å­˜åœ¨çš„è¯¥ URL è®°å½• (é¿å…é‡å¤)
        history = [h for h in history if h.get('url') != req.url]
        
        # æ„å»ºæ–°è®°å½• (åŒ…å«åç§°å’Œæµé‡å¿«ç…§)
        new_record = {
            "url": req.url,
            "date": datetime.now().strftime('%Y-%m-%d %H:%M'),
            "name": airport_name,
            "info": traffic_info
        }
        history.insert(0, new_record)
        
        if len(history) > 10: history = history[:10]
        data['sub_history'] = history
        
        # ä¿å­˜åˆ°æ–‡ä»¶ (åŒ…å« user_info çš„æ›´æ–°)
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(data, indent=2))
            
    except Exception as e:
        logger.error(f"å¤„ç†è®¢é˜…å‡ºé”™: {e}")
        # å‡ºé”™æ—¶ä¹Ÿè¦å°è¯•ä¿å­˜ä¸‹ URLï¼Œé˜²æ­¢ç”¨æˆ·ä¸¢å¤±è¾“å…¥
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(data, indent=2))
        raise HTTPException(status_code=500, detail=f"Processing Error: {str(e)}")
        
    return {"status": "success"}

@app.get("/api/analysis")
async def analyze_config():
    if not os.path.exists(OUTPUT_YAML) or os.path.getsize(OUTPUT_YAML) == 0:
        return {"status": "empty", "groups": [], "rules": [], "rule_count": 0, "regions": []}
    
    try:
        async with aiofiles.open(OUTPUT_YAML, 'r', encoding='utf-8') as f:
            content = await f.read()
            config = yaml.safe_load(content)
            if not config: return {"status": "empty"}
        
        json_rules_map = {}
        try:
            if os.path.exists(CONFIG_JSON):
                async with aiofiles.open(CONFIG_JSON, 'r') as f:
                    content = await f.read()
                    saved_data = json.loads(content)
                    for r in saved_data.get('add_rules', []):
                        clean = clean_rule_for_clash(r)
                        json_rules_map[clean] = r
        except: pass

        rule_usage = Counter()
        final_display_rules = []
        
        for r in config.get('rules', []):
            target = get_rule_target(r)
            if target: rule_usage[target] += 1
            if r in json_rules_map:
                final_display_rules.append(json_rules_map[r])
            else:
                final_display_rules.append(r)

        groups_info = [{"name": g['name'], "type": g.get('type', 'select'), "rule_count": rule_usage.get(g['name'], 0)} for g in config.get('proxy-groups', [])]
        
        proxies = config.get('proxies'ï¼Œ [])
        region_map = {
            "hk": "é¦™æ¸¯"ï¼Œ "hong": "é¦™æ¸¯", "é¦™æ¸¯": "é¦™æ¸¯",
            "tw": "å°æ¹¾"ï¼Œ "tai": "å°æ¹¾", "å°æ¹¾": "å°æ¹¾",
            "jp": "æ—¥æœ¬", "japan": "æ—¥æœ¬", "æ—¥æœ¬": "æ—¥æœ¬",
            "us": "ç¾å›½", "america": "ç¾å›½", "united": "ç¾å›½", "ç¾å›½": "ç¾å›½",
            "sg": "æ–°åŠ å¡", "sing": "æ–°åŠ å¡", "æ–°åŠ å¡": "æ–°åŠ å¡",
            "kr": "éŸ©å›½", "korea": "éŸ©å›½", "éŸ©å›½": "éŸ©å›½",
            "uk": "è‹±å›½", "gb": "è‹±å›½", "è‹±å›½": "è‹±å›½",
            "de": "å¾·å›½", "ger": "å¾·å›½", "å¾·å›½": "å¾·å›½",
            "ca": "åŠ æ‹¿å¤§", "can": "åŠ æ‹¿å¤§", "åŠ æ‹¿å¤§": "åŠ æ‹¿å¤§",
            "tr": "åœŸè€³å…¶", "tur": "åœŸè€³å…¶", "åœŸ": "åœŸè€³å…¶",
            "fr": "æ³•å›½", "france": "æ³•å›½", "æ³•": "æ³•å›½",
            "ru": "ä¿„ç½—æ–¯"ï¼Œ "russia": "ä¿„ç½—æ–¯", "ä¿„": "ä¿„ç½—æ–¯"
        }
        icons = {
            "é¦™æ¸¯": "ğŸ‡­ğŸ‡°"ï¼Œ "å°æ¹¾": "ğŸ‡¹ğŸ‡¼", "æ—¥æœ¬": "ğŸ‡¯ğŸ‡µ", "ç¾å›½": "ğŸ‡ºğŸ‡¸", 
            "æ–°åŠ å¡": "ğŸ‡¸ğŸ‡¬"ï¼Œ "éŸ©å›½": "ğŸ‡°ğŸ‡·", "è‹±å›½": "ğŸ‡¬ğŸ‡§", "å¾·å›½": "ğŸ‡©ğŸ‡ª", 
            "åŠ æ‹¿å¤§": "ğŸ‡¨ğŸ‡¦", "åœŸè€³å…¶": "ğŸ‡¹ğŸ‡·", "æ³•å›½": "ğŸ‡«ğŸ‡·", "ä¿„ç½—æ–¯": "ğŸ‡·ğŸ‡º", "å…¶ä»–": "ğŸŒ"
        }
        
        counts = {}
        for p in proxies:
            name = p.get('name', '').lower()
            found = False
            for k, v åœ¨ region_map.items():
                if k åœ¨ name:
                    if v not åœ¨ counts: counts[v] = {"name": v, "icon": icons.get(v, "ğŸŒ"), "count": 0}
                    counts[v]['count'] += 1
                    found = True
                    break
            if not found:
                if "å…¶ä»–" not åœ¨ counts: counts["å…¶ä»–"] = {"name": "å…¶ä»–", "icon": "ğŸŒ", "count": 0}
                counts["å…¶ä»–"]['count'] += 1
        
        regions = sorted(counts.values(), key=lambda x: x['count'], reverse=True)
        final_regions = [r for r åœ¨ regions if r['name'] != 'å…¶ä»–']
        if "å…¶ä»–" åœ¨ counts: final_regions.append(counts["å…¶ä»–"])

        mtime = os.pathã€‚getmtime(OUTPUT_YAML)
        ts_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            "status": "success", 
            "groups": groups_info, 
            "rules": final_display_rules, 
            "rule_count": len(final_display_rules), 
            "regions": final_regions, 
            "total_nodes": len(proxies), 
            "update_time": ts_str,
            "ts": datetime.å½“å‰()ã€‚timestamp()
        }
    except Exception as e: return {"status": "error", "msg": str(e)}

# --- é™æ€æ–‡ä»¶æŒ‚è½½ ---
if os.path.exists("images"):
    app.mount("/images"ï¼Œ StaticFiles(directory="images"), name="images")

app.mount("/", StaticFiles(directory="static", html=True), name="static")
