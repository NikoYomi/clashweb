import os
import json
import yaml
import httpx
import logging
import docker
import asyncio
import re
from datetime import datetime
from urllib.parse import quote
from typing import List, Dict, Any, Optional
from collections import Counter
import pytz

# æ£€æŸ¥ä¾èµ–
try:
    import aiofiles
except ImportError:
    print("CRITICAL ERROR: ç¼ºå°‘ 'aiofiles' åº“ã€‚è¯·ç¡®ä¿ requirements.txt ä¸­åŒ…å« aiofiles å¹¶é‡æ–°æ„å»ºé•œåƒï¼")
    exit(1)

from fastapi import FastAPI, HTTPException, UploadFile, File, Request, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# [Scheduler]
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# --- é…ç½®è·¯å¾„ ---
DATA_PATH = "/data"
CONFIG_JSON = os.path.join(DATA_PATH, "config.json")
OUTPUT_YAML = os.path.join(DATA_PATH, "config.yaml")
LOG_FILE = os.path.join(DATA_PATH, "app.log")
DEFAULT_BACKEND = "https://api.v1.mk/sub?target=clash&url="

# --- åˆå§‹åŒ–æ—¥å¿— ---
logger = logging.getLogger("ClashWeb")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)

if not os.path.exists(DATA_PATH):
    try:
        os.makedirs(DATA_PATH)
    except:
        pass
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# --- åˆå§‹åŒ–è°ƒåº¦å™¨ ---
tz = pytz.timezone('Asia/Shanghai')
scheduler = AsyncIOScheduler(timezone=tz)

app = FastAPI(title="ClashWeb")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- æ•°æ®æ¨¡å‹ ---

class SubHistoryItem(BaseModel):
    url: str
    date: str
    remarks: Optional[str] = ""

class UserInfo(BaseModel):
    upload: int = 0
    download: int = 0
    total: int = 0
    expire: int = 0
    update_time: str = ""

class ConfigModel(BaseModel):
    sub_backend: Optional[str] = ""
    sub_url: Optional[str] = ""
    restart_containers: Optional[str] = "" 
    auto_update: Optional[bool] = False
    cron_expression: Optional[str] = "0 4 * * *" 
    user_info: Optional[UserInfo] = UserInfo()
    sub_history: Optional[List[SubHistoryItem]] = []
    add_groups: Optional[List[Dict[str, Any]]] = []
    del_groups: Optional[List[str]] = []
    add_rules: Optional[List[str]] = []
    del_rules: Optional[List[str]] = []

class DownloadRequest(BaseModel):
    url: str

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def init_data():
    if not os.path.exists(DATA_PATH):
        try:
            os.makedirs(DATA_PATH)
        except: pass
    
    if not os.path.exists(CONFIG_JSON) or os.path.isdir(CONFIG_JSON):
        with open(CONFIG_JSON, 'w') as f: json.dump(ConfigModel().dict(), f)

    if not os.path.exists(OUTPUT_YAML) or os.path.isdir(OUTPUT_YAML):
        with open(OUTPUT_YAML, 'w') as f: f.write("")

def refresh_scheduler():
    try:
        with open(CONFIG_JSON, 'r') as f:
            data = json.load(f)
        
        job_id = 'auto_update_job'
        if scheduler.get_job(job_id):
            scheduler.remove_job(job_id)

        if data.get('auto_update') and data.get('cron_expression'):
            cron_str = data['cron_expression']
            try:
                trigger = CronTrigger.from_crontab(cron_str, timezone=tz)
                scheduler.add_job(scheduled_update_task, trigger, id=job_id, replace_existing=True)
                logger.info(f"âœ… å®šæ—¶ä»»åŠ¡å·²è®¾ç½®: [{cron_str}]")
            except Exception as e:
                logger.error(f"Invalid cron expression: {e}")
        else:
            logger.info("â›”ï¸ å®šæ—¶ä»»åŠ¡å·²å…³é—­")
    except Exception as e:
        logger.error(f"Scheduler refresh failed: {e}")

async def scheduled_update_task():
    logger.info(">>> â³ å¼€å§‹æ‰§è¡Œå®šæ—¶æ›´æ–°ä»»åŠ¡ <<<")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
        
        url = data.get('sub_url')
        if not url:
            logger.warning("æœªé…ç½®è®¢é˜…é“¾æ¥ï¼Œè·³è¿‡æ›´æ–°")
            return

        # æ‰§è¡Œæ›´æ–°é€»è¾‘
        await internal_process_subscription(url, data)
        
        logger.info("âœ… å®šæ—¶æ›´æ–°ä»»åŠ¡å®Œæˆ")

        container_str = data.get('restart_containers', '')
        if container_str:
            try:
                client = docker.from_env()
                targets = [name.strip() for name in container_str.split(',') if name.strip()]
                for name in targets:
                    try:
                        client.containers.get(name).restart()
                        logger.info(f"âœ… å®¹å™¨å·²é‡å¯: {name}")
                    except Exception as e:
                        logger.error(f"âŒ é‡å¯å®¹å™¨ {name} å¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"Docker è¿æ¥å¤±è´¥: {e}")
                    
    except Exception as e:
        logger.error(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")

# --- é€»è¾‘åˆ†ç¦»ï¼šä»»åŠ¡1 è·å–åŸå§‹æµé‡ä¿¡æ¯ ---
async def fetch_original_userinfo(url: str) -> Optional[dict]:
    """ç›´æ¥è¯·æ±‚åŸå§‹è®¢é˜…é“¾æ¥ï¼Œä»…æå– Headerï¼Œä¸ä¸‹è½½ Body"""
    logger.info(f"ğŸ“¡ [æµé‡ä»»åŠ¡] æ­£åœ¨ç›´æ¥è¯·æ±‚åŸå§‹é“¾æ¥è·å– Header: {url}")
    headers = {"User-Agent": "ClashForAndroid/2.5.12"} 
    
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            # ä½¿ç”¨ GET ä½†é€šè¿‡ stream ç«‹å³å…³é—­ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶ï¼Œç±»ä¼¼äº HEAD ä½†å…¼å®¹æ€§æ›´å¥½
            async with client.stream("GET", url, headers=headers, timeout=30.0) as resp:
                # æ‰“å° Headers è°ƒè¯•
                # logger.info(f"åŸå§‹é“¾æ¥å“åº”å¤´: {resp.headers}")
                
                user_info_header = None
                for k, v in resp.headers.items():
                    if k.lower() == 'subscription-userinfo':
                        user_info_header = v
                        break
                
                if user_info_header:
                    info = {}
                    parts = user_info_header.split(';')
                    for part in parts:
                        if '=' in part:
                            kv = part.strip().split('=')
                            if len(kv) >= 2:
                                info[kv[0].strip()] = int(kv[1].strip())
                    
                    result = {
                        "upload": info.get("upload", 0),
                        "download": info.get("download", 0),
                        "total": info.get("total", 0),
                        "expire": info.get("expire", 0),
                        "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    logger.info(f"âœ… [æµé‡ä»»åŠ¡] æˆåŠŸè·å–: {result}")
                    return result
                else:
                    logger.warning("âš ï¸ [æµé‡ä»»åŠ¡] åŸå§‹é“¾æ¥æœªè¿”å› subscription-userinfo")
                    return None
    except Exception as e:
        logger.warning(f"âŒ [æµé‡ä»»åŠ¡] è¯·æ±‚å¤±è´¥: {e}")
        return None

# --- é€»è¾‘åˆ†ç¦»ï¼šä»»åŠ¡2 ä¸‹è½½å¹¶è½¬æ¢é…ç½® ---
async def download_and_convert_config(url: str, data: dict) -> bool:
    """è¯·æ±‚è½¬æ¢åç«¯ï¼Œä¸‹è½½ YAMLï¼Œå¹¶åº”ç”¨ Patch"""
    base_url = data.get('sub_backend') or DEFAULT_BACKEND
    if "target=" not in base_url:
        if not base_url.endswith("/"): base_url += "/"
        base_url += "sub?target=clash&url="
    
    encoded_sub_url = quote(url, safe='') 
    full_url = f"{base_url}{encoded_sub_url}"
    
    # è½¬æ¢åç«¯é€šå¸¸æ¨¡æ‹Ÿ Chrome
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    logger.info(f"â¬‡ï¸ [ä¸‹è½½ä»»åŠ¡] æ­£åœ¨è¯·æ±‚è½¬æ¢åç«¯: {full_url}")
    
    config_yaml = ""
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            resp = await client.get(full_url, headers=headers, timeout=60.0)
            if resp.status_code != 200:
                raise Exception(f"è½¬æ¢åç«¯è¿”å›é”™è¯¯ç : {resp.status_code}")
            
            config_yaml = resp.content.decode('utf-8', errors='ignore')
            if "No nodes were found" in config_yaml:
                raise Exception("åç«¯è¿”å› 'No nodes were found'ï¼Œè¯·æ£€æŸ¥è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] ä¸‹è½½å¤±è´¥: {e}")
        raise e

    # è§£æ YAML
    try:
        config = yaml.safe_load(config_yaml)
        if not isinstance(config, dict):
            raise Exception("è§£æç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] YAML è§£æå¤±è´¥: {e}")
        raise Exception("YAML è§£æå¤±è´¥ï¼Œå†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ Clash é…ç½®")

    # åº”ç”¨è¡¥ä¸ (Patch)
    try:
        final_config = apply_patch(config, data)
        
        # å¼ºåˆ¶ Block Style å†™å…¥ï¼Œé˜²æ­¢ä¹±ç 
        output_str = yaml.dump(final_config, allow_unicode=True, sort_keys=False, default_flow_style=False, width=float("inf"))
        
        # æ ¡éªŒ
        yaml.safe_load(output_str)
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] é…ç½®å¤„ç†æˆ–æ ¡éªŒå¤±è´¥: {e}")
        raise Exception(f"é…ç½®å¤„ç†å¤±è´¥: {e}")

    # å†™å…¥æ–‡ä»¶
    async with aiofiles.open(OUTPUT_YAML, 'w', encoding='utf-8') as f:
        await f.write(output_str)
    
    logger.info("âœ… [ä¸‹è½½ä»»åŠ¡] é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ config.yaml")
    return True

# --- ä¸»æµç¨‹ ---
async def internal_process_subscription(url: str, data: dict):
    """
    å¹¶å‘æ‰§è¡Œï¼š
    1. ä»åŸå§‹é“¾æ¥è·å–æµé‡ä¿¡æ¯ (ä¸å½±å“é…ç½®ç”Ÿæˆ)
    2. ä»è½¬æ¢åç«¯è·å–é…ç½®æ–‡ä»¶ (æ ¸å¿ƒä»»åŠ¡)
    """
    
    # åˆ›å»ºä¸¤ä¸ªä»»åŠ¡
    task_traffic = fetch_original_userinfo(url)
    task_download = download_and_convert_config(url, data)
    
    # å¹¶å‘æ‰§è¡Œï¼Œreturn_exceptions=True ç¡®ä¿ä¸€ä¸ªå¤±è´¥ä¸ä¼šä¸­æ–­å¦ä¸€ä¸ª
    results = await asyncio.gather(task_traffic, task_download, return_exceptions=True)
    
    fetched_user_info = results[0]
    download_result = results[1]
    
    # å¤„ç†æµé‡ä¿¡æ¯ç»“æœ
    if isinstance(fetched_user_info, dict):
        data['user_info'] = fetched_user_info
        # æ›´æ–° config.json
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(data, indent=2))
    elif isinstance(fetched_user_info, Exception):
        # æµé‡è·å–å¤±è´¥ä»…è®°å½•æ—¥å¿—ï¼Œä¸æŠ›å‡ºå¼‚å¸¸é˜»æ–­æµç¨‹
        logger.warning(f"æµé‡ä¿¡æ¯è·å–ä»»åŠ¡å¼‚å¸¸: {fetched_user_info}")

    # å¤„ç†ä¸‹è½½ç»“æœ
    if isinstance(download_result, Exception):
        # ä¸‹è½½å¤±è´¥å¿…é¡»æŠ›å‡ºå¼‚å¸¸ç»™å‰ç«¯
        raise download_result

def get_rule_target(rule_str: str) -> str:
    try:
        clean = rule_str.split('#')[0].strip()
        parts = clean.split(',')
        if len(parts) >= 3:
            return parts[2].strip()
    except: pass
    return ""

def clean_rule_for_clash(rule_str: str) -> str:
    return rule_str.split('#')[0].strip()

def apply_patch(config: dict, patch: dict) -> dict:
    config['allow-lan'] = True
    config['external-controller'] = '0.0.0.0:9090'
    if 'bind-address' in config: config['bind-address'] = '*'

    reference_proxies = ["DIRECT", "REJECT"]
    source_groups = config.get('proxy-groups', [])
    for g in source_groups:
        if g.get('type') == 'select' and len(g.get('proxies', [])) > 3:
            reference_proxies = g['proxies']
            break

    del_groups_list = patch.get('del_groups') or []
    add_rules_raw = patch.get('add_rules') or []

    if del_groups_list:
        config['proxy-groups'] = [g for g in config.get('proxy-groups', []) if g['name'] not in del_groups_list]
        new_base_rules = []
        for rule in config.get('rules', []):
            if get_rule_target(rule) not in del_groups_list:
                new_base_rules.append(rule)
        config['rules'] = new_base_rules
        
        valid_add_rules = []
        for rule in add_rules_raw:
            if get_rule_target(rule) not in del_groups_list:
                valid_add_rules.append(rule)
        add_rules_raw = valid_add_rules

    add_groups = patch.get('add_groups') or []
    if add_groups:
        existing_names = {g['name'] for g in config.get('proxy-groups', [])}
        for g in reversed(add_groups):
            if g.get('name') and g['name'] not in existing_names:
                new_group = g.copy()
                current_proxies = new_group.get('proxies', [])
                if not current_proxies or current_proxies == ["DIRECT", "REJECT"]:
                     new_group['proxies'] = list(reference_proxies)
                config.setdefault('proxy-groups', []).insert(0, new_group)

    del_keywords = patch.get('del_rules') or []
    if del_keywords:
        final_rules = []
        for rule in config.get('rules', []):
            clean_rule = clean_rule_for_clash(rule)
            if not any(k in clean_rule for k in del_keywords): 
                final_rules.append(rule)
        config['rules'] = final_rules

    if add_rules_raw:
        for r in reversed(add_rules_raw): 
            clean_r = clean_rule_for_clash(r)
            if clean_r:
                config.setdefault('rules', []).insert(0, clean_r)
             
    return config

@app.on_event("startup")
async def startup_event():
    init_data()
    scheduler.start()
    refresh_scheduler()
    logger.info("Application started, scheduler running.")

@app.get("/api/logs")
async def get_logs(lines: int = 100):
    if not os.path.exists(LOG_FILE):
        return {"logs": []}
    try:
        async with aiofiles.open(LOG_FILE, 'r', encoding='utf-8') as f:
            content = await f.read()
            all_lines = content.splitlines()
            return {"logs": all_lines[-lines:]}
    except Exception as e:
        return {"logs": [f"Error reading logs: {str(e)}"]}

@app.get("/api/data")
async def get_data():
    try:
        if os.path.exists(CONFIG_JSON) and os.path.getsize(CONFIG_JSON) > 0:
            async with aiofiles.open(CONFIG_JSON, 'r') as f:
                content = await f.read()
                data = json.loads(content)
                if 'user_info' not in data:
                    data['user_info'] = {"upload":0, "download":0, "total":0, "expire":0, "update_time": ""}
                return data
        return {}
    except: return {}

@app.post("/api/data")
async def save_data(data: ConfigModel):
    try:
        payload = data.dict(exclude_none=True)
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(payload, indent=2))
        
        refresh_scheduler()
        return {"status": "success"}
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/backup")
async def backup_config(include_sub: bool = False):
    if not os.path.exists(CONFIG_JSON): raise HTTPException(status_code=404, detail="No config found")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
            
        if not include_sub:
            data['sub_url'] = ""
            data['sub_history'] = []
            
        temp_path = "/tmp/clashweb_backup.json"
        async with aiofiles.open(temp_path, 'w') as f:
            await f.write(json.dumps(data, indent=2))
            
        return FileResponse(temp_path, filename="clashweb_backup.json", media_type="application/json")
    except Exception as e:
        raise HTTPException(500, detail=str(e))

@app.post("/api/restore")
async def restore_config(file: UploadFile = File(...), restore_sub: bool = Form(False)):
    try:
        content = await file.read()
        backup_data = json.loads(content)
        if not isinstance(backup_data, dict): raise ValueError("Format Error")
        
        final_data = backup_data
        if not restore_sub:
            current_data = {}
            if os.path.exists(CONFIG_JSON):
                with open(CONFIG_JSON, 'r') as f: current_data = json.load(f)
            final_data['sub_url'] = current_data.get('sub_url', '')
            final_data['sub_history'] = current_data.get('sub_history', [])
        
        if restore_sub and not final_data.get('sub_url'):
             raise ValueError("å¤‡ä»½æ–‡ä»¶ä¸­æœªåŒ…å«è®¢é˜…ä¿¡æ¯")

        async with aiofiles.open(CONFIG_JSON, "w") as f:
            await f.write(json.dumps(final_data, indent=2))
        
        refresh_scheduler()
        
        summary = {
            "groups": len(final_data.get('add_groups', [])),
            "rules": len(final_data.get('add_rules', [])),
            "sub_status": "å·²è¦†ç›–" if restore_sub else "æœªå˜æ›´",
            "has_sub": bool(final_data.get('sub_url'))
        }
        return {"status": "success", "summary": summary}
    except Exception as e:
        logger.error(f"è¿˜åŸå¤±è´¥: {e}")
        raise HTTPException(status_code=400, detail=f"Restore Failed: {str(e)}")

@app.post("/api/restart_containers")
async def restart_containers():
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
            
        targets = [n.strip() for n in data.get('restart_containers', '').split(',') if n.strip()]
        if not targets: raise HTTPException(400, detail="æœªè®¾ç½®å®¹å™¨")
        
        client = docker.from_env()
        restarted = []
        for name in targets:
            try:
                client.containers.get(name).restart()
                restarted.append(name)
                logger.info(f"æ‰‹åŠ¨è§¦å‘ - å®¹å™¨å·²é‡å¯: {name}")
            except Exception as e:
                logger.error(f"æ‰‹åŠ¨è§¦å‘ - é‡å¯å¤±è´¥ {name}: {e}")
            
        if not restarted:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æœ‰æ•ˆå®¹å™¨")
            
        return {"status": "success", "msg": f"å·²é‡å¯: {', '.join(restarted)}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Docker Error: {str(e)}")

@app.post("/api/download")
async def download_config(req: DownloadRequest):
    if not req.url: raise HTTPException(status_code=400, detail="Missing URL")

    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
    except: data = {}
    
    data['sub_url'] = req.url
    history = data.get('sub_history', [])
    # ç®€å•çš„å†å²å»é‡é€»è¾‘
    history = [h for h in history if h['url'] != req.url]
    history.insert(0, {"url": req.url, "date": datetime.now().strftime('%Y-%m-%d %H:%M')})
    if len(history) > 10: history = history[:10]
    data['sub_history'] = history
    
    async with aiofiles.open(CONFIG_JSON, 'w') as f:
        await f.write(json.dumps(data, indent=2))

    try:
        # è°ƒç”¨æ–°çš„é€»è¾‘
        await internal_process_subscription(req.url, data)
    except Exception as e:
        logger.error(f"å¤„ç†è®¢é˜…å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"Processing Error: {str(e)}")
        
    return {"status": "success"}

@app.get("/api/analysis")
async def analyze_config():
    if not os.path.exists(OUTPUT_YAML) or os.path.getsize(OUTPUT_YAML) == 0:
        return {"status": "empty", "groups": [], "rules": [], "rule_count": 0, "regions": []}
    
    try:
        async with aiofiles.open(OUTPUT_YAML, 'r', encoding='utf-8') as f:
            content = await f.read()
            config = yaml.safe_load(content)
            if not config: return {"status": "empty"}
        
        json_rules_map = {}
        try:
            if os.path.exists(CONFIG_JSON):
                async with aiofiles.open(CONFIG_JSON, 'r') as f:
                    content = await f.read()
                    saved_data = json.loads(content)
                    for r in saved_data.get('add_rules', []):
                        clean = clean_rule_for_clash(r)
                        json_rules_map[clean] = r
        except: pass

        rule_usage = Counter()
        final_display_rules = []
        
        for r in config.get('rules', []):
            target = get_rule_target(r)
            if target: rule_usage[target] += 1
            if r in json_rules_map:
                final_display_rules.append(json_rules_map[r])
            else:
                final_display_rules.append(r)

        groups_info = [{"name": g['name'], "type": g.get('type', 'select'), "rule_count": rule_usage.get(g['name'], 0)} for g in config.get('proxy-groups', [])]
        
        proxies = config.get('proxies', [])
        region_map = {
            "hk": "é¦™æ¸¯", "hong": "é¦™æ¸¯", "é¦™æ¸¯": "é¦™æ¸¯",
            "tw": "å°æ¹¾", "tai": "å°æ¹¾", "å°æ¹¾": "å°æ¹¾",
            "jp": "æ—¥æœ¬", "japan": "æ—¥æœ¬", "æ—¥æœ¬": "æ—¥æœ¬",
            "us": "ç¾å›½", "america": "ç¾å›½", "united": "ç¾å›½", "ç¾å›½": "ç¾å›½",
            "sg": "æ–°åŠ å¡", "sing": "æ–°åŠ å¡", "æ–°åŠ å¡": "æ–°åŠ å¡",
            "kr": "éŸ©å›½", "korea": "éŸ©å›½", "éŸ©å›½": "éŸ©å›½",
            "uk": "è‹±å›½", "gb": "è‹±å›½", "è‹±å›½": "è‹±å›½",
            "de": "å¾·å›½", "ger": "å¾·å›½", "å¾·å›½": "å¾·å›½",
            "ca": "åŠ æ‹¿å¤§", "can": "åŠ æ‹¿å¤§", "åŠ æ‹¿å¤§": "åŠ æ‹¿å¤§",
            "tr": "åœŸè€³å…¶", "tur": "åœŸè€³å…¶", "åœŸ": "åœŸè€³å…¶",
            "fr": "æ³•å›½", "france": "æ³•å›½", "æ³•": "æ³•å›½",
            "ru": "ä¿„ç½—æ–¯", "russia": "ä¿„ç½—æ–¯", "ä¿„": "ä¿„ç½—æ–¯"
        }
        icons = {
            "é¦™æ¸¯": "ğŸ‡­ğŸ‡°", "å°æ¹¾": "ğŸ‡¹ğŸ‡¼", "æ—¥æœ¬": "ğŸ‡¯ğŸ‡µ", "ç¾å›½": "ğŸ‡ºğŸ‡¸", 
            "æ–°åŠ å¡": "ğŸ‡¸ğŸ‡¬", "éŸ©å›½": "ğŸ‡°ğŸ‡·", "è‹±å›½": "ğŸ‡¬ğŸ‡§", "å¾·å›½": "ğŸ‡©ğŸ‡ª", 
            "åŠ æ‹¿å¤§": "ğŸ‡¨ğŸ‡¦", "åœŸè€³å…¶": "ğŸ‡¹ğŸ‡·", "æ³•å›½": "ğŸ‡«ğŸ‡·", "ä¿„ç½—æ–¯": "ğŸ‡·ğŸ‡º", "å…¶ä»–": "ğŸŒ"
        }
        
        counts = {}
        for p in proxies:
            name = p.get('name', '').lower()
            found = False
            for k, v in region_map.items():
                if k in name:
                    if v not in counts: counts[v] = {"name": v, "icon": icons.get(v, "ğŸŒ"), "count": 0}
                    counts[v]['count'] += 1
                    found = True
                    break
            if not found:
                if "å…¶ä»–" not in counts: counts["å…¶ä»–"] = {"name": "å…¶ä»–", "icon": "ğŸŒ", "count": 0}
                counts["å…¶ä»–"]['count'] += 1
        
        regions = sorted(counts.values(), key=lambda x: x['count'], reverse=True)
        final_regions = [r for r in regions if r['name'] != 'å…¶ä»–']
        if "å…¶ä»–" in counts: final_regions.append(counts["å…¶ä»–"])

        mtime = os.path.getmtime(OUTPUT_YAML)
        ts_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            "status": "success", 
            "groups": groups_info, 
            "rules": final_display_rules, 
            "rule_count": len(final_display_rules), 
            "regions": final_regions, 
            "total_nodes": len(proxies), 
            "update_time": ts_str,
            "ts": datetime.now().timestamp()
        }
    except Exception as e: return {"status": "error", "msg": str(e)}

# --- é™æ€æ–‡ä»¶æŒ‚è½½ ---
if os.path.exists("images"):
    app.mount("/images", StaticFiles(directory="images"), name="images")

app.mount("/", StaticFiles(directory="static", html=True), name="static")