import os
import json
import yaml
import httpx
import logging
import docker
import asyncio
import re
from datetime import datetime
from urllib.parse import quote, unquote, urlparse, parse_qs
from typing import List, Dict, Any, Optional
from collections import Counter
import pytz

# æ£€æŸ¥ä¾èµ–
try:
    import aiofiles
except ImportError:
    print("CRITICAL ERROR: ç¼ºå°‘ 'aiofiles' åº“ã€‚è¯·ç¡®ä¿ requirements.txt ä¸­åŒ…å« aiofiles å¹¶é‡æ–°æ„å»ºé•œåƒï¼")
    exit(1)

from fastapi import FastAPI, HTTPException, UploadFile, File, Request, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# [Scheduler]
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# --- é…ç½®è·¯å¾„ ---
DATA_PATH = "/data"
CONFIG_JSON = os.path.join(DATA_PATH, "config.json")
OUTPUT_YAML = os.path.join(DATA_PATH, "config.yaml")
LOG_FILE = os.path.join(DATA_PATH, "app.log")
DEFAULT_BACKEND = "https://api.v1.mk/sub?target=clash&url="

# --- åˆå§‹åŒ–æ—¥å¿— ---
logger = logging.getLogger("ClashWeb")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)

if not os.path.exists(DATA_PATH):
    try:
        os.makedirs(DATA_PATH)
    except:
        pass
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# --- åˆå§‹åŒ–è°ƒåº¦å™¨ ---
tz = pytz.timezone('Asia/Shanghai')
scheduler = AsyncIOScheduler(timezone=tz)

app = FastAPI(title="ClashWeb")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- æ•°æ®æ¨¡å‹ ---

class SubHistoryItem(BaseModel):
    name: Optional[str] = "" 
    url: str
    date: str
    remarks: Optional[str] = ""
    web_url: Optional[str] = "" 
    upload: int = 0
    download: int = 0
    total: int = 0
    expire: int = 0

class UserInfo(BaseModel):
    name: str = ""  
    web_url: str = "" 
    upload: int = 0
    download: int = 0
    total: int = 0
    expire: int = 0
    update_time: str = ""

class ConfigModel(BaseModel):
    sub_backend: Optional[str] = ""
    sub_url: Optional[str] = ""
    restart_containers: Optional[str] = "" 
    auto_update: Optional[bool] = False
    cron_expression: Optional[str] = "0 4 * * *" 
    user_info: Optional[UserInfo] = UserInfo()
    sub_history: Optional[List[SubHistoryItem]] = []
    add_groups: Optional[List[Dict[str, Any]]] = []
    del_groups: Optional[List[str]] = []
    add_rules: Optional[List[str]] = []
    del_rules: Optional[List[str]] = []

class DownloadRequest(BaseModel):
    url: str

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def init_data():
    if not os.path.exists(DATA_PATH):
        try:
            os.makedirs(DATA_PATH)
        except: pass
    
    if not os.path.exists(CONFIG_JSON) or os.path.isdir(CONFIG_JSON):
        with open(CONFIG_JSON, 'w') as f: json.dump(ConfigModel().dict(), f)

    if not os.path.exists(OUTPUT_YAML) or os.path.isdir(OUTPUT_YAML):
        with open(OUTPUT_YAML, 'w') as f: f.write("")

def refresh_scheduler():
    try:
        with open(CONFIG_JSON, 'r') as f:
            data = json.load(f)
        
        job_id = 'auto_update_job'
        if scheduler.get_job(job_id):
            scheduler.remove_job(job_id)

        if data.get('auto_update') and data.get('cron_expression'):
            cron_str = data['cron_expression']
            try:
                trigger = CronTrigger.from_crontab(cron_str, timezone=tz)
                scheduler.add_job(scheduled_update_task, trigger, id=job_id, replace_existing=True)
                logger.info(f"âœ… å®šæ—¶ä»»åŠ¡å·²è®¾ç½®: [{cron_str}]")
            except Exception as e:
                logger.error(f"Invalid cron expression: {e}")
        else:
            logger.info("â›”ï¸ å®šæ—¶ä»»åŠ¡å·²å…³é—­")
    except Exception as e:
        logger.error(f"Scheduler refresh failed: {e}")

async def scheduled_update_task():
    logger.info(">>> â³ å¼€å§‹æ‰§è¡Œå®šæ—¶æ›´æ–°ä»»åŠ¡ <<<")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
        
        url = data.get('sub_url')
        if not url:
            logger.warning("æœªé…ç½®è®¢é˜…é“¾æ¥ï¼Œè·³è¿‡æ›´æ–°")
            return

        await internal_process_subscription(url, data)
        
        logger.info("âœ… å®šæ—¶æ›´æ–°ä»»åŠ¡å®Œæˆ")

        container_str = data.get('restart_containers', '')
        if container_str:
            try:
                client = docker.from_env()
                targets = [name.strip() for name in container_str.split(',') if name.strip()]
                for name in targets:
                    try:
                        client.containers.get(name).restart()
                        logger.info(f"âœ… å®¹å™¨å·²é‡å¯: {name}")
                    except Exception as e:
                        logger.error(f"âŒ é‡å¯å®¹å™¨ {name} å¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"Docker è¿æ¥å¤±è´¥: {e}")
                    
    except Exception as e:
        logger.error(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")

# --- è¾…åŠ©å‡½æ•°ï¼šä» HTML æå– Title ---
def extract_title_from_html(html_content: str) -> Optional[str]:
    try:
        title_match = re.search(r'<title>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        if title_match:
            title = title_match.group(1).strip()
            # ç®€å•æ¸…æ´—ï¼šå¦‚æœæ ‡é¢˜å¤ªé•¿æˆ–è€…æ˜¯é”™è¯¯é¡µé¢ï¼Œåˆ™è®¤ä¸ºæ— æ•ˆ
            if title and len(title) < 50 and "404" not in title and "Error" not in title:
                return title
    except: pass
    return None

# --- é€»è¾‘åˆ†ç¦»ï¼šä»»åŠ¡1 æ™ºèƒ½è·å–ä¿¡æ¯ ---
async def fetch_original_userinfo(url: str) -> Optional[dict]:
    """æ™ºèƒ½åˆ†æï¼šæµé‡ + æ–‡ä»¶åè§£æ + å®˜ç½‘æ ‡é¢˜æŠ“å–(æ”¯æŒä¸»åŸŸåå›é€€)"""
    logger.info(f"ğŸ“¡ [æµé‡ä»»åŠ¡] åˆ†æè®¢é˜…: {url}")
    headers = {"User-Agent": "ClashForAndroid/2.5.12"} 
    
    # 1. åŸºç¡€è§£æ
    parsed_uri = urlparse(url)
    current_host = parsed_uri.netloc
    web_url = f"{parsed_uri.scheme}://{current_host}"
    
    # å°è¯•è®¡ç®—ä¸»åŸŸå (ä¾‹å¦‚ sub.a.com -> a.com)
    root_url = None
    host_parts = current_host.split('.')
    if len(host_parts) > 2 and not re.match(r'^\d+\.\d+\.\d+\.\d+$', current_host):
        # ç®€å•çš„å–åä¸¤æ®µä½œä¸ºä¸»åŸŸå (é€‚ç”¨äº .com, .net ç­‰ï¼Œå¯¹ .co.uk å¯èƒ½ä¸å‡†ä½†è¶³å¤Ÿç”¨)
        root_domain = ".".join(host_parts[-2:])
        root_url = f"{parsed_uri.scheme}://{root_domain}"

    # 2. é»˜è®¤åç§°å…œåº•
    fallback_name = "æœªçŸ¥è®¢é˜…"
    if parsed_uri.fragment: fallback_name = unquote(parsed_uri.fragment)
    else:
        qs = parse_qs(parsed_uri.query)
        if 'name' in qs: fallback_name = qs['name'][0]
        elif 'remarks' in qs: fallback_name = qs['remarks'][0]
        else: fallback_name = current_host

    sub_name = fallback_name
    info = {}

    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            # --- é˜¶æ®µ A: è¯·æ±‚è®¢é˜…é“¾æ¥ (æ‹¿æµé‡ + å“åº”å¤´æ–‡ä»¶å) ---
            try:
                async with client.stream("GET", url, headers=headers, timeout=30.0) as resp:
                    # å¢å¼ºç‰ˆ Content-Disposition è§£æ
                    cd = resp.headers.get("content-disposition", "")
                    if cd:
                        # ä¼˜å…ˆå°è¯• filename*=utf-8''xxx æ ¼å¼
                        fn_star = re.search(r"filename\*=UTF-8''(.+)", cd, re.IGNORECASE)
                        if fn_star:
                            sub_name = unquote(fn_star.group(1))
                        else:
                            # å°è¯• filename="xxx"
                            fn_quote = re.search(r'filename="(.+?)"', cd, re.IGNORECASE)
                            if fn_quote:
                                sub_name = unquote(fn_quote.group(1))
                            else:
                                # å°è¯• filename=xxx
                                fn_simple = re.search(r'filename=([^;]+)', cd, re.IGNORECASE)
                                if fn_simple:
                                    sub_name = unquote(fn_simple.group(1).strip().strip('"'))
                        
                        # æ¸…ç†åç¼€
                        if sub_name and sub_name != fallback_name:
                            if sub_name.lower().endswith(('.yaml', '.yml', '.conf', '.txt')):
                                sub_name = sub_name.rsplit('.', 1)[0]

                    # è§£ææµé‡å¤´
                    user_info_header = None
                    for k, v in resp.headers.items():
                        if k.lower() == 'subscription-userinfo':
                            user_info_header = v
                            break
                    if user_info_header:
                        parts = user_info_header.split(';')
                        for part in parts:
                            if '=' in part:
                                kv = part.strip().split('=')
                                if len(kv) >= 2: info[kv[0].strip()] = int(kv[1].strip())
            except Exception as e:
                logger.warning(f"è®¢é˜…é“¾æ¥è¯·æ±‚å¼‚å¸¸: {e}")

            # --- é˜¶æ®µ B: å¦‚æœåå­—æœªè·å–ï¼Œå°è¯•çˆ¬å–å®˜ç½‘æ ‡é¢˜ ---
            if sub_name == fallback_name or sub_name == current_host:
                browser_headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                }
                
                # ç­–ç•¥ B1: è®¿é—®å½“å‰åŸŸå (å¦‚ sub.site.com)
                title_found = False
                if web_url:
                    logger.info(f"ğŸ•µï¸ å°è¯•è®¿é—®: {web_url}")
                    try:
                        r = await client.get(web_url, headers=browser_headers, timeout=5.0)
                        if r.status_code == 200:
                            t = extract_title_from_html(r.text[:20000])
                            if t: 
                                sub_name = t
                                title_found = True
                                logger.info(f"âœ… ä»å­åŸŸåè·å–æ ‡é¢˜: {t}")
                    except: pass

                # ç­–ç•¥ B2: å¦‚æœB1å¤±è´¥ï¼Œä¸”æœ‰ä¸»åŸŸåï¼Œè®¿é—®ä¸»åŸŸå (å¦‚ site.com)
                if not title_found and root_url and root_url != web_url:
                    logger.info(f"ğŸ•µï¸ å°è¯•å›é€€è®¿é—®ä¸»åŸŸå: {root_url}")
                    try:
                        r = await client.get(root_url, headers=browser_headers, timeout=5.0)
                        if r.status_code == 200:
                            t = extract_title_from_html(r.text[:20000])
                            if t: 
                                sub_name = t
                                # å…³é”®ï¼šæ›´æ–°å®˜ç½‘åœ°å€ä¸ºä¸»åŸŸåï¼Œä¿®å¤ç‚¹å‡»è·³è½¬
                                web_url = root_url 
                                logger.info(f"âœ… ä»ä¸»åŸŸåè·å–æ ‡é¢˜: {t}")
                    except: pass

            result = {
                "name": sub_name, 
                "web_url": web_url,
                "upload": info.get("upload", 0),
                "download": info.get("download", 0),
                "total": info.get("total", 0),
                "expire": info.get("expire", 0),
                "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            return result

    except Exception as e:
        logger.error(f"âŒ è·å–ä¿¡æ¯æµç¨‹å¤±è´¥: {e}")
        return {
            "name": fallback_name, "web_url": web_url,
            "upload": 0, "download": 0, "total": 0, "expire": 0,
            "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

# --- é€»è¾‘åˆ†ç¦»ï¼šä»»åŠ¡2 ä¸‹è½½å¹¶è½¬æ¢é…ç½® ---
async def download_and_convert_config(url: str, data: dict) -> bool:
    """è¯·æ±‚è½¬æ¢åç«¯ï¼Œä¸‹è½½ YAMLï¼Œå¹¶åº”ç”¨ Patch"""
    base_url = data.get('sub_backend') or DEFAULT_BACKEND
    if "target=" not in base_url:
        if not base_url.endswith("/"): base_url += "/"
        base_url += "sub?target=clash&url="
    
    encoded_sub_url = quote(url, safe='') 
    full_url = f"{base_url}{encoded_sub_url}"
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    logger.info(f"â¬‡ï¸ [ä¸‹è½½ä»»åŠ¡] æ­£åœ¨è¯·æ±‚è½¬æ¢åç«¯: {full_url}")
    
    config_yaml = ""
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            resp = await client.get(full_url, headers=headers, timeout=60.0)
            if resp.status_code != 200:
                raise Exception(f"è½¬æ¢åç«¯è¿”å›é”™è¯¯ç : {resp.status_code}")
            
            config_yaml = resp.content.decode('utf-8', errors='ignore')
            if "No nodes were found" in config_yaml:
                raise Exception("åç«¯è¿”å› 'No nodes were found'ï¼Œè¯·æ£€æŸ¥è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] ä¸‹è½½å¤±è´¥: {e}")
        raise e

    try:
        config = yaml.safe_load(config_yaml)
        if not isinstance(config, dict):
            raise Exception("è§£æç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] YAML è§£æå¤±è´¥: {e}")
        raise Exception("YAML è§£æå¤±è´¥ï¼Œå†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ Clash é…ç½®")

    try:
        final_config = apply_patch(config, data)
        output_str = yaml.dump(final_config, allow_unicode=True, sort_keys=False, default_flow_style=False, width=float("inf"))
        yaml.safe_load(output_str)
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] é…ç½®å¤„ç†æˆ–æ ¡éªŒå¤±è´¥: {e}")
        raise Exception(f"é…ç½®å¤„ç†å¤±è´¥: {e}")

    async with aiofiles.open(OUTPUT_YAML, 'w', encoding='utf-8') as f:
        await f.write(output_str)
    
    logger.info("âœ… [ä¸‹è½½ä»»åŠ¡] é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ config.yaml")
    return True

# --- ä¸»æµç¨‹ ---
async def internal_process_subscription(url: str, data: dict):
    task_traffic = fetch_original_userinfo(url)
    task_download = download_and_convert_config(url, data)
    
    results = await asyncio.gather(task_traffic, task_download, return_exceptions=True)
    
    fetched_user_info = results[0]
    download_result = results[1]
    
    if isinstance(fetched_user_info, dict):
        data['user_info'] = fetched_user_info
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(data, indent=2))
    elif isinstance(fetched_user_info, Exception):
        logger.warning(f"æµé‡ä¿¡æ¯è·å–ä»»åŠ¡å¼‚å¸¸: {fetched_user_info}")

    if isinstance(download_result, Exception):
        raise download_result

def get_rule_target(rule_str: str) -> str:
    try:
        clean = rule_str.split('#')[0].strip()
        parts = clean.split(',')
        if len(parts) >= 3:
            return parts[2].strip()
    except: pass
    return ""

def clean_rule_for_clash(rule_str: str) -> str:
    return rule_str.split('#')[0].strip()

def apply_patch(config: dict, patch: dict) -> dict:
    config['allow-lan'] = True
    config['external-controller'] = '0.0.0.0:9090'
    if 'bind-address' in config: config['bind-address'] = '*'

    reference_proxies = ["DIRECT", "REJECT"]
    source_groups = config.get('proxy-groups', [])
    for g in source_groups:
        if g.get('type') == 'select' and len(g.get('proxies', [])) > 3:
            reference_proxies = g['proxies']
            break

    del_groups_list = patch.get('del_groups') or []
    add_rules_raw = patch.get('add_rules') or []

    if del_groups_list:
        config['proxy-groups'] = [g for g in config.get('proxy-groups', []) if g['name'] not in del_groups_list]
        new_base_rules = []
        for rule in config.get('rules', []):
            if get_rule_target(rule) not in del_groups_list:
                new_base_rules.append(rule)
        config['rules'] = new_base_rules
        
        valid_add_rules = []
        for rule in add_rules_raw:
            if get_rule_target(rule) not in del_groups_list:
                valid_add_rules.append(rule)
        add_rules_raw = valid_add_rules

    add_groups = patch.get('add_groups') or []
    if add_groups:
        existing_names = {g['name'] for g in config.get('proxy-groups', [])}
        for g in reversed(add_groups):
            if g.get('name') and g['name'] not in existing_names:
                new_group = g.copy()
                current_proxies = new_group.get('proxies', [])
                if not current_proxies or current_proxies == ["DIRECT", "REJECT"]:
                     new_group['proxies'] = list(reference_proxies)
                config.setdefault('proxy-groups', []).insert(0, new_group)

    del_keywords = patch.get('del_rules') or []
    if del_keywords:
        final_rules = []
        for rule in config.get('rules', []):
            clean_rule = clean_rule_for_clash(rule)
            if not any(k in clean_rule for k in del_keywords): 
                final_rules.append(rule)
        config['rules'] = final_rules

    if add_rules_raw:
        for r in reversed(add_rules_raw): 
            clean_r = clean_rule_for_clash(r)
            if clean_r:
                config.setdefault('rules', []).insert(0, clean_r)
             
    return config

@app.on_event("startup")
async def startup_event():
    init_data()
    scheduler.start()
    refresh_scheduler()
    logger.info("Application started, scheduler running.")

@app.get("/api/logs")
async def get_logs(lines: int = 100):
    if not os.path.exists(LOG_FILE):
        return {"logs": []}
    try:
        async with aiofiles.open(LOG_FILE, 'r', encoding='utf-8') as f:
            content = await f.read()
            all_lines = content.splitlines()
            return {"logs": all_lines[-lines:]}
    except Exception as e:
        return {"logs": [f"Error reading logs: {str(e)}"]}

@app.get("/api/data")
async def get_data():
    try:
        if os.path.exists(CONFIG_JSON) and os.path.getsize(CONFIG_JSON) > 0:
            async with aiofiles.open(CONFIG_JSON, 'r') as f:
                content = await f.read()
                data = json.loads(content)
                if 'user_info' not in data:
                    data['user_info'] = {"name": "", "web_url": "", "upload":0, "download":0, "total":0, "expire":0, "update_time": ""}
                return data
        return {}
    except: return {}

@app.post("/api/data")
async def save_data(data: ConfigModel):
    try:
        payload = data.dict(exclude_none=True)
        async with aiofiles.open(CONFIG_JSON, 'w') as f:
            await f.write(json.dumps(payload, indent=2))
        
        refresh_scheduler()
        return {"status": "success"}
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/backup")
async def backup_config(include_sub: bool = False):
    if not os.path.exists(CONFIG_JSON): raise HTTPException(status_code=404, detail="No config found")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
            
        if not include_sub:
            data['sub_url'] = ""
            data['sub_history'] = []
            
        temp_path = "/tmp/clashweb_backup.json"
        async with aiofiles.open(temp_path, 'w') as f:
            await f.write(json.dumps(data, indent=2))
            
        return FileResponse(temp_path, filename="clashweb_backup.json", media_type="application/json")
    except Exception as e:
        raise HTTPException(500, detail=str(e))

@app.post("/api/restore")
async def restore_config(file: UploadFile = File(...), restore_sub: bool = Form(False)):
    try:
        content = await file.read()
        backup_data = json.loads(content)
        if not isinstance(backup_data, dict): raise ValueError("Format Error")
        
        final_data = backup_data
        if not restore_sub:
            current_data = {}
            if os.path.exists(CONFIG_JSON):
                with open(CONFIG_JSON, 'r') as f: current_data = json.load(f)
            final_data['sub_url'] = current_data.get('sub_url', '')
            final_data['sub_history'] = current_data.get('sub_history', [])
        
        if restore_sub and not final_data.get('sub_url'):
             raise ValueError("å¤‡ä»½æ–‡ä»¶ä¸­æœªåŒ…å«è®¢é˜…ä¿¡æ¯")

        async with aiofiles.open(CONFIG_JSON, "w") as f:
            await f.write(json.dumps(final_data, indent=2))
        
        refresh_scheduler()
        
        summary = {
            "groups": len(final_data.get('add_groups', [])),
            "rules": len(final_data.get('add_rules', [])),
            "sub_status": "å·²è¦†ç›–" if restore_sub else "æœªå˜æ›´",
            "has_sub": bool(final_data.get('sub_url'))
        }
        return {"status": "success", "summary": summary}
    except Exception as e:
        logger.error(f"è¿˜åŸå¤±è´¥: {e}")
        raise HTTPException(status_code=400, detail=f"Restore Failed: {str(e)}")

@app.post("/api/restart_containers")
async def restart_containers():
    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
            
        targets = [n.strip() for n in data.get('restart_containers', '').split(',') if n.strip()]
        if not targets: raise HTTPException(400, detail="æœªè®¾ç½®å®¹å™¨")
        
        client = docker.from_env()
        restarted = []
        for name in targets:
            try:
                client.containers.get(name).restart()
                restarted.append(name)
                logger.info(f"æ‰‹åŠ¨è§¦å‘ - å®¹å™¨å·²é‡å¯: {name}")
            except Exception as e:
                logger.error(f"æ‰‹åŠ¨è§¦å‘ - é‡å¯å¤±è´¥ {name}: {e}")
            
        if not restarted:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æœ‰æ•ˆå®¹å™¨")
            
        return {"status": "success", "msg": f"å·²é‡å¯: {', '.join(restarted)}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Docker Error: {str(e)}")

@app.post("/api/download")
async def download_config(req: DownloadRequest):
    if not req.url: raise HTTPException(status_code=400, detail="Missing URL")

    try:
        async with aiofiles.open(CONFIG_JSON, 'r') as f:
            content = await f.read()
            data = json.loads(content)
    except: data = {}
    
    try:
        await internal_process_subscription(req.url, data)
    except Exception as e:
        logger.error(f"å¤„ç†è®¢é˜…å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"Processing Error: {str(e)}")

    u_info = data.get('user_info', {})
    
    history = data.get('sub_history', [])
    history = [h for h in history if h['url'] != req.url]
    history.insert(0, {
        "name": u_info.get('name', 'æœªçŸ¥è®¢é˜…'),
        "web_url": u_info.get('web_url', ''), 
        "url": req.url, 
        "date": datetime.now().strftime('%Y-%m-%d %H:%M'),
        "upload": u_info.get('upload', 0),    
        "download": u_info.get('download', 0),
        "total": u_info.get('total', 0),
        "expire": u_info.get('expire', 0)
    })
    if len(history) > 10: history = history[:10]
    data['sub_history'] = history
    data['sub_url'] = req.url
    
    async with aiofiles.open(CONFIG_JSON, 'w') as f:
        await f.write(json.dumps(data, indent=2))
        
    return {"status": "success"}

@app.get("/api/analysis")
async def analyze_config():
    if not os.path.exists(OUTPUT_YAML) or os.path.getsize(OUTPUT_YAML) == 0:
        return {"status": "empty", "groups": [], "rules": [], "rule_count": 0, "regions": []}
    
    try:
        async with aiofiles.open(OUTPUT_YAML, 'r', encoding='utf-8') as f:
            content = await f.read()
            config = yaml.safe_load(content)
            if not config: return {"status": "empty"}
        
        json_rules_map = {}
        try:
            if os.path.exists(CONFIG_JSON):
                async with aiofiles.open(CONFIG_JSON, 'r') as f:
                    content = await f.read()
                    saved_data = json.loads(content)
                    for r in saved_data.get('add_rules', []):
                        clean = clean_rule_for_clash(r)
                        json_rules_map[clean] = r
        except: pass

        rule_usage = Counter()
        final_display_rules = []
        
        for r in config.get('rules', []):
            target = get_rule_target(r)
            if target: rule_usage[target] += 1
            if r in json_rules_map:
                final_display_rules.append(json_rules_map[r])
            else:
                final_display_rules.append(r)

        groups_info = [{"name": g['name'], "type": g.get('type', 'select'), "rule_count": rule_usage.get(g['name'], 0)} for g in config.get('proxy-groups', [])]
        
        proxies = config.get('proxies', [])
        region_map = {
            "hk": "é¦™æ¸¯", "hong": "é¦™æ¸¯", "é¦™æ¸¯": "é¦™æ¸¯",
            "tw": "å°æ¹¾", "tai": "å°æ¹¾", "å°æ¹¾": "å°æ¹¾",
            "jp": "æ—¥æœ¬", "japan": "æ—¥æœ¬", "æ—¥æœ¬": "æ—¥æœ¬",
            "us": "ç¾å›½", "america": "ç¾å›½", "united": "ç¾å›½", "ç¾å›½": "ç¾å›½",
            "sg": "æ–°åŠ å¡", "sing": "æ–°åŠ å¡", "æ–°åŠ å¡": "æ–°åŠ å¡",
            "kr": "éŸ©å›½", "korea": "éŸ©å›½", "éŸ©å›½": "éŸ©å›½",
            "uk": "è‹±å›½", "gb": "è‹±å›½", "è‹±å›½": "è‹±å›½",
            "de": "å¾·å›½", "ger": "å¾·å›½", "å¾·å›½": "å¾·å›½",
            "ca": "åŠ æ‹¿å¤§", "can": "åŠ æ‹¿å¤§", "åŠ æ‹¿å¤§": "åŠ æ‹¿å¤§",
            "tr": "åœŸè€³å…¶", "tur": "åœŸè€³å…¶", "åœŸ": "åœŸè€³å…¶",
            "fr": "æ³•å›½", "france": "æ³•å›½", "æ³•": "æ³•å›½",
            "ru": "ä¿„ç½—æ–¯"ï¼Œ "russia": "ä¿„ç½—æ–¯", "ä¿„": "ä¿„ç½—æ–¯",
            "vn": "è¶Šå—"ï¼Œ "viet": "è¶Šå—", "è¶Šå—": "è¶Šå—",
            "ae": "é˜¿è”é…‹"ï¼Œ "uae": "é˜¿è”é…‹", "é˜¿è”é…‹": "é˜¿è”é…‹", "dubai": "è¿ªæ‹œ", "è¿ªæ‹œ": "è¿ªæ‹œ",
            "my": "é©¬æ¥è¥¿äºš"ï¼Œ "mal": "é©¬æ¥è¥¿äºš", "é©¬æ¥è¥¿äºš": "é©¬æ¥è¥¿äºš",
            "th": "æ³°å›½", "thai": "æ³°å›½", "æ³°å›½": "æ³°å›½",
            "kh": "æŸ¬åŸ”å¯¨", "cam": "æŸ¬åŸ”å¯¨", "æŸ¬åŸ”å¯¨": "æŸ¬åŸ”å¯¨",
            "br": "å·´è¥¿", "bra": "å·´è¥¿", "å·´è¥¿": "å·´è¥¿",
            "au": "æ¾³å¤§åˆ©äºš", "aus": "æ¾³å¤§åˆ©äºš", "æ¾³å¤§åˆ©äºš": "æ¾³å¤§åˆ©äºš",
            "in": "å°åº¦", "ind": "å°åº¦", "å°åº¦": "å°åº¦",
            "id": "å°åº¦å°¼è¥¿äºš", "indo": "å°åº¦å°¼è¥¿äºš", "å°åº¦å°¼è¥¿äºš": "å°åº¦å°¼è¥¿äºš",
            "nl": "è·å…°", "net": "è·å…°", "è·å…°": "è·å…°",
            "ch": "ç‘å£«", "swi": "ç‘å£«", "ç‘å£«": "ç‘å£«"
        }
        icons = {
            "é¦™æ¸¯": "ğŸ‡­ğŸ‡°"ï¼Œ "å°æ¹¾": "ğŸ‡¹ğŸ‡¼", "æ—¥æœ¬": "ğŸ‡¯ğŸ‡µ", "ç¾å›½": "ğŸ‡ºğŸ‡¸",
            "æ–°åŠ å¡": "ğŸ‡¸ğŸ‡¬"ï¼Œ "éŸ©å›½": "ğŸ‡°ğŸ‡·", "è‹±å›½": "ğŸ‡¬ğŸ‡§", "å¾·å›½": "ğŸ‡©ğŸ‡ª",
            "åŠ æ‹¿å¤§": "ğŸ‡¨ğŸ‡¦"ï¼Œ "åœŸè€³å…¶": "ğŸ‡¹ğŸ‡·", "æ³•å›½": "ğŸ‡«ğŸ‡·", "ä¿„ç½—æ–¯": "ğŸ‡·ğŸ‡º",
            "è¶Šå—": "ğŸ‡»ğŸ‡³"ï¼Œ "é˜¿è”é…‹": "ğŸ‡¦ğŸ‡ª", "è¿ªæ‹œ": "ğŸ‡¦ğŸ‡ª", "é©¬æ¥è¥¿äºš": "ğŸ‡²ğŸ‡¾", "æ³°å›½": "ğŸ‡¹ğŸ‡­",
            "æŸ¬åŸ”å¯¨": "ğŸ‡°ğŸ‡­", "å·´è¥¿": "ğŸ‡§ğŸ‡·", "æ¾³å¤§åˆ©äºš": "ğŸ‡¦ğŸ‡º", "å°åº¦": "ğŸ‡®ğŸ‡³",
            "å°åº¦å°¼è¥¿äºš": "ğŸ‡®ğŸ‡©", "è·å…°": "ğŸ‡³ğŸ‡±", "ç‘å£«": "ğŸ‡¨ğŸ‡­", "å…¶ä»–": "ğŸŒ"
        }
        
        counts = {}
        for p in proxies:
            name = p.get('name', '').lower()
            found = False
            for k, v åœ¨ region_map.items():
                if k åœ¨ name:
                    if v not in counts: counts[v] = {"name": v, "icon": icons.get(v, "ğŸŒ"), "count": 0}
                    counts[v]['count'] += 1
                    found = True
                    break
            if not found:
                if "å…¶ä»–" not åœ¨ counts: counts["å…¶ä»–"] = {"name": "å…¶ä»–", "icon": "ğŸŒ", "count": 0}
                counts["å…¶ä»–"]['count'] += 1
        
        regions = sorted(counts.values(), key=lambda x: x['count'], reverse=True)
        final_regions = [r for r åœ¨ regions if r['name'] != 'å…¶ä»–']
        if "å…¶ä»–" in counts: final_regions.append(counts["å…¶ä»–"])

        mtime = os.pathã€‚getmtime(OUTPUT_YAML)
        ts_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            "status": "success"ï¼Œ 
            "groups": groups_info, 
            "rules": final_display_rules, 
            "rule_count": len(final_display_rules), 
            "regions": final_regions, 
            "total_nodes": len(proxies), 
            "update_time": ts_str,
            "ts": datetime.now().timestamp()
        }
    except Exception as e: return {"status": "error", "msg": str(e)}

if os.path.exists("images"):
    app.mount("/images"ï¼Œ StaticFiles(directory="images"), name="images")

app.mount("/", StaticFiles(directory="static", html=True), name="static")
