import os
import json
import yaml
import httpx
import logging
import docker
import asyncio
import re
from datetime import datetime
from urllib.parse import quote, unquote, urlparse
from typing import List, Dict, Any, Optional
from collections import Counter
import pytz

# æ£€æŸ¥ä¾èµ–
try:
    import aiofiles
except ImportError:
    print("CRITICAL ERROR: ç¼ºå°‘ 'aiofiles' åº“ã€‚è¯·ç¡®ä¿ requirements.txt ä¸­åŒ…å« aiofiles å¹¶é‡æ–°æ„å»ºé•œåƒï¼")
    exit(1)

from fastapi import FastAPI, HTTPException, UploadFile, File, Request, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# [Scheduler]
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# --- é…ç½®è·¯å¾„ ---
DATA_PATH = "/data"
CONFIG_JSON = os.path.join(DATA_PATH, "config.json")
OUTPUT_YAML = os.path.join(DATA_PATH, "config.yaml")
DEFAULT_BACKEND = "https://api.v1.mk/sub?target=clash&url="

# --- åˆå§‹åŒ–æ—¥å¿— ---
LOG_FILE = os.path.join(DATA_PATH, "app.log")
logger = logging.getLogger("ClashWeb")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)

if not os.path.exists(DATA_PATH):
    try:
        os.makedirs(DATA_PATH)
    except:
        pass

# ä½¿ç”¨ utf-8 ç¼–ç åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# --- åˆå§‹åŒ–è°ƒåº¦å™¨ ---
tz = pytz.timezone('Asia/Shanghai')
scheduler = AsyncIOScheduler(timezone=tz)

app = FastAPI(title="ClashWeb")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- æ•°æ®æ¨¡å‹ ---

class UserInfo(BaseModel):
    name: str = "æœªè®¢é˜…" 
    webUrl: str = "" 
    upload: int = 0
    download: int = 0
    total: int = 0
    expire: int = 0
    update_time: str = ""

class SubHistoryItem(BaseModel):
    url: str
    date: str
    name: Optional[str] = "æœªçŸ¥æœºåœº"
    info: Optional[Dict[str, Any]] = {}
    remarks: Optional[str] = ""

class ConfigModel(BaseModel):
    sub_backend: Optional[str] = ""
    sub_url: Optional[str] = ""
    restart_containers: Optional[str] = "" 
    auto_update: Optional[bool] = False
    cron_expression: Optional[str] = "0 4 * * *" 
    user_info: Optional[UserInfo] = UserInfo()
    sub_history: Optional[List[SubHistoryItem]] = []
    add_groups: Optional[List[Dict[str, Any]]] = []
    del_groups: Optional[List[str]] = []
    add_rules: Optional[List[str]] = []
    del_rules: Optional[List[str]] = []

class DownloadRequest(BaseModel):
    url: str

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def init_data():
    if not os.path.exists(DATA_PATH):
        try:
            os.makedirs(DATA_PATH)
        except: pass
    
    if not os.path.exists(CONFIG_JSON) or os.path.isdir(CONFIG_JSON):
        with open(CONFIG_JSON, 'w', encoding='utf-8') as f: json.dump(ConfigModel().dict(), f)

    if not os.path.exists(OUTPUT_YAML) or os.path.isdir(OUTPUT_YAML):
        with open(OUTPUT_YAML, 'w', encoding='utf-8') as f: f.write("")

def refresh_scheduler():
    try:
        with open(CONFIG_JSON, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        job_id = 'auto_update_job'
        if scheduler.get_job(job_id):
            scheduler.remove_job(job_id)

        if data.get('auto_update') and data.get('cron_expression'):
            cron_str = data['cron_expression']
            try:
                # å°è¯•éªŒè¯ Cron è¡¨è¾¾å¼
                trigger = CronTrigger.from_crontab(cron_str, timezone=tz)
                scheduler.add_job(scheduled_update_task, trigger, id=job_id, replace_existing=True)
                logger.info(f"âœ… å®šæ—¶ä»»åŠ¡å·²è®¾ç½®: [{cron_str}]")
            except Exception as e:
                # æ•è·æ— æ•ˆè¡¨è¾¾å¼ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
                logger.error(f"âŒ Cron è¡¨è¾¾å¼æ— æ•ˆ '{cron_str}': {e}ã€‚å®šæ—¶ä»»åŠ¡æœªå¯åŠ¨ã€‚å»ºè®®æ£€æŸ¥è¡¨è¾¾å¼æ ¼å¼ (å¦‚ '0 4 * * *')")
        else:
            logger.info("â›”ï¸ å®šæ—¶ä»»åŠ¡å·²å…³é—­")
    except Exception as e:
        logger.error(f"Scheduler refresh failed: {e}")

async def scheduled_update_task():
    logger.info(">>> â³ å¼€å§‹æ‰§è¡Œå®šæ—¶æ›´æ–°ä»»åŠ¡ <<<")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)
        
        url = data.get('sub_url')
        if not url:
            logger.warning("æœªé…ç½®è®¢é˜…é“¾æ¥ï¼Œè·³è¿‡æ›´æ–°")
            return

        # æ‰§è¡Œæ›´æ–°é€»è¾‘
        await internal_process_subscription(url, data)
        
        # ä¿å­˜ user_info æ›´æ–°
        async with aiofiles.open(CONFIG_JSON, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2))

        logger.info("âœ… å®šæ—¶æ›´æ–°ä»»åŠ¡å®Œæˆ")

        # è‡ªåŠ¨é‡å¯å®¹å™¨é€»è¾‘ (å…¼å®¹ä¸­æ–‡é€—å·)
        container_str = data.get('restart_containers', '').replace('ï¼Œ', ',')
        if container_str:
            try:
                client = docker.from_env()
                targets = [name.strip() for name in container_str.split(',') if name.strip()]
                for name in targets:
                    try:
                        client.containers.get(name).restart()
                        logger.info(f"âœ… (å®šæ—¶) å®¹å™¨å·²é‡å¯: {name}")
                    except Exception as e:
                        logger.error(f"âŒ (å®šæ—¶) é‡å¯å®¹å™¨ {name} å¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"Docker è¿æ¥å¤±è´¥: {e}")
                    
    except Exception as e:
        logger.error(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")

# --- ä»»åŠ¡1: è·å–åŸå§‹æµé‡ä¿¡æ¯ ---
async def fetch_original_userinfo(url: str) -> Optional[dict]:
    """ç›´æ¥è¯·æ±‚åŸå§‹è®¢é˜…é“¾æ¥ï¼Œæå– Header ä¸­çš„æµé‡ä¿¡æ¯ã€profile-title å’Œå®˜ç½‘åœ°å€"""
    logger.info(f"ğŸ“¡ [ä¿¡æ¯è·å–] æ­£åœ¨è¯·æ±‚åŸå§‹é“¾æ¥: {url}")
    headers = {"User-Agent": "ClashForAndroid/2.5.12"} 
    
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            # ä½¿ç”¨ GET ä½†é€šè¿‡ stream ç«‹å³å…³é—­ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶
            async with client.stream("GET", url, headers=headers, timeout=30.0) as resp:
                
                # 1. æå–æµé‡ä¿¡æ¯
                user_info_header = None
                for k, v in resp.headers.items():
                    if k.lower() == 'subscription-userinfo':
                        user_info_header = v
                        break
                
                info = {}
                if user_info_header:
                    parts = user_info_header.split(';')
                    for part in parts:
                        if '=' in part:
                            kv = part.strip().split('=')
                            if len(kv) >= 2:
                                info[kv[0].strip()] = int(kv[1].strip())

                # 2. æå–æœºåœºåç§°
                airport_name = ""
                # A. ä¼˜å…ˆæ£€æŸ¥ profile-title
                for k, v in resp.headers.items():
                    if k.lower() == 'profile-title':
                        try: airport_name = unquote(v)
                        except: airport_name = v
                        break
                
                # B. Content-Disposition æå– (å¢å¼ºå…¼å®¹æ€§)
                if not airport_name:
                    for k, v in resp.headers.items():
                        if k.lower() == 'content-disposition':
                            m = re.search(r'filename\*?=(?:UTF-8\'\')?([^;]+)', v, re.IGNORECASE)
                            if m:
                                raw_name = m.group(1).strip('"\'')
                                try:
                                    airport_name = unquote(raw_name)
                                    if '.' in airport_name: airport_name = airport_name.rsplit('.', 1)[0]
                                except: pass
                            break
                
                # C. åŸŸåå…œåº•
                if not airport_name:
                    try: airport_name = urlparse(url).netloc
                    except: airport_name = "æœªçŸ¥è®¢é˜…"

                # 3. æå–å®˜ç½‘åœ°å€ (webUrl)
                web_url = ""
                # A. å°è¯•ä»å“åº”å¤´è·å– (Clash æ ‡å‡†å¤´)
                for k, v in resp.headers.items():
                    if k.lower() == 'profile-web-page-url':
                        web_url = v.strip()
                        break
                
                # B. åŸŸåå…œåº•ï¼šå¦‚æœå¤´ä¿¡æ¯æ²¡æœ‰ï¼Œä½¿ç”¨ subscription url çš„ root domain
                if not web_url:
                    try:
                        parsed = urlparse(url)
                        web_url = f"{parsed.scheme}://{parsed.netloc}"
                    except: pass

                result = {
                    "name": airport_name,
                    "webUrl": web_url,
                    "upload": info.get("upload", 0),
                    "download": info.get("download", 0),
                    "total": info.get("total", 0),
                    "expire": info.get("expire", 0),
                    "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                logger.info(f"âœ… [ä¿¡æ¯è·å–] æˆåŠŸ: {result}")
                return result

    except Exception as e:
        logger.warning(f"âŒ [ä¿¡æ¯è·å–] è¯·æ±‚å¤±è´¥: {e}")
        return None

# --- ä»»åŠ¡2: ä¸‹è½½å¹¶è½¬æ¢é…ç½® ---
async def download_and_convert_config(url: str, data: dict) -> bool:
    """è¯·æ±‚è½¬æ¢åç«¯ï¼Œä¸‹è½½ YAMLï¼Œå¹¶åº”ç”¨ Patch"""
    base_url = data.get('sub_backend') or DEFAULT_BACKEND
    if "target=" not in base_url:
        if not base_url.endswith("/"): base_url += "/"
        base_url += "sub?target=clash&url="
    
    encoded_sub_url = quote(url, safe='') 
    full_url = f"{base_url}{encoded_sub_url}"
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    logger.info(f"â¬‡ï¸ [ä¸‹è½½ä»»åŠ¡] æ­£åœ¨è¯·æ±‚è½¬æ¢åç«¯: {full_url}")
    
    config_yaml = ""
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True) as client:
            resp = await client.get(full_url, headers=headers, timeout=60.0)
            if resp.status_code != 200:
                raise Exception(f"è½¬æ¢åç«¯è¿”å›é”™è¯¯ç : {resp.status_code}")
            
            config_yaml = resp.content.decode('utf-8', errors='ignore')
            if "No nodes were found" in config_yaml:
                raise Exception("åç«¯è¿”å› 'No nodes were found'ï¼Œè¯·æ£€æŸ¥è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] ä¸‹è½½å¤±è´¥: {e}")
        raise e

    # è§£æ YAML
    try:
        config = yaml.safe_load(config_yaml)
        if not isinstance(config, dict):
            raise Exception("è§£æç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] YAML è§£æå¤±è´¥: {e}")
        raise Exception("YAML è§£æå¤±è´¥ï¼Œå†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ Clash é…ç½®")

    # åº”ç”¨è¡¥ä¸ (Patch)
    try:
        final_config = apply_patch(config, data)
        # å¼ºåˆ¶å…è®¸ Unicodeï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
        output_str = yaml.dump(final_config, allow_unicode=True, sort_keys=False, default_flow_style=False, width=float("inf"))
        yaml.safe_load(output_str) # æ ¡éªŒ
    except Exception as e:
        logger.error(f"âŒ [ä¸‹è½½ä»»åŠ¡] é…ç½®å¤„ç†æˆ–æ ¡éªŒå¤±è´¥: {e}")
        raise Exception(f"é…ç½®å¤„ç†å¤±è´¥: {e}")

    # å†™å…¥æ–‡ä»¶
    async with aiofiles.open(OUTPUT_YAML, 'w', encoding='utf-8') as f:
        await f.write(output_str)
    
    logger.info("âœ… [ä¸‹è½½ä»»åŠ¡] é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ config.yaml")
    return True

# --- ä¸»æµç¨‹ ---
async def internal_process_subscription(url: str, data: dict) -> Optional[dict]:
    """
    å¹¶å‘æ‰§è¡Œï¼š1.è·å–æµé‡ 2.ä¸‹è½½é…ç½®
    """
    task_traffic = fetch_original_userinfo(url)
    task_download = download_and_convert_config(url, data)
    
    results = await asyncio.gather(task_traffic, task_download, return_exceptions=True)
    
    fetched_user_info = results[0]
    download_result = results[1]
    
    # å¤„ç†æµé‡ä¿¡æ¯ç»“æœ
    if isinstance(fetched_user_info, dict):
        data['user_info'] = fetched_user_info
    elif isinstance(fetched_user_info, Exception):
        logger.warning(f"æµé‡ä¿¡æ¯è·å–ä»»åŠ¡å¼‚å¸¸: {fetched_user_info}")

    # å¤„ç†ä¸‹è½½ç»“æœ
    if isinstance(download_result, Exception):
        raise download_result

    return fetched_user_info if isinstance(fetched_user_info, dict) else None

def get_rule_target(rule_str: str) -> str:
    try:
        clean = rule_str.split('#')[0].strip()
        parts = clean.split(',')
        if len(parts) >= 3:
            return parts[2].strip()
    except: pass
    return ""

def clean_rule_for_clash(rule_str: str) -> str:
    # ç®€å•çš„æ¸…ç†ï¼Œä¸»è¦ç”¨äºæ¯”å¯¹
    return rule_str.split('#')[0].strip()

# --- Patch é€»è¾‘ (å…³é”®ä¿®æ­£ï¼šç¡®ä¿è‡ªå®šä¹‰å†…å®¹ç”Ÿæ•ˆ) ---
def apply_patch(config: dict, patch: dict) -> dict:
    config['allow-lan'] = True
    config['external-controller'] = '0.0.0.0:9090'
    if 'bind-address' in config: config['bind-address'] = '*'

    # ç¡®å®šå‚è€ƒèŠ‚ç‚¹ (ç”¨äºæ–°ç»„é»˜è®¤å¡«å……)
    reference_proxies = ["DIRECT", "REJECT"]
    source_groups = config.get('proxy-groups', [])
    for g in source_groups:
        if g.get('type') == 'select' and len(g.get('proxies', [])) > 3:
            reference_proxies = g['proxies']
            break

    # [åˆ é™¤ç»„é€»è¾‘]ï¼šä½¿ç”¨ strip() ç¡®ä¿ç²¾å‡†åŒ¹é…
    del_groups_list = [n.strip() for n in (patch.get('del_groups') or []) if n.strip()]
    add_rules_raw = patch.get('add_rules') or []

    if del_groups_list:
        # è¿‡æ»¤ç»„
        config['proxy-groups'] = [
            g for g in config.get('proxy-groups', []) 
            if g['name'].strip() not in del_groups_list
        ]
        
        # çº§è”åˆ é™¤ï¼šå¦‚æœè§„åˆ™æŒ‡å‘äº†å·²åˆ é™¤çš„ç»„ï¼Œåˆ™è¯¥è§„åˆ™ä¹Ÿåˆ é™¤
        new_base_rules = []
        for rule in config.get('rules', []):
            target = get_rule_target(rule)
            if target not in del_groups_list:
                new_base_rules.append(rule)
        config['rules'] = new_base_rules
        
        # åŒæ ·è¿‡æ»¤ç”¨æˆ·æ–°å¢çš„è§„åˆ™
        valid_add_rules = []
        for rule in add_rules_raw:
            target = get_rule_target(rule)
            if target not in del_groups_list:
                valid_add_rules.append(rule)
        add_rules_raw = valid_add_rules

    # [æ·»åŠ ç»„é€»è¾‘]ï¼šæ’å…¥åˆ°æœ€å‰
    add_groups = patch.get('add_groups') or []
    if add_groups:
        existing_names = {g['name'] for g in config.get('proxy-groups', [])}
        for g in reversed(add_groups):
            if g.get('name') and g['name'] not in existing_names:
                new_group = g.copy()
                current_proxies = new_group.get('proxies', [])
                if not current_proxies or current_proxies == ["DIRECT", "REJECT"]:
                     new_group['proxies'] = list(reference_proxies)
                config.setdefault('proxy-groups', []).insert(0, new_group)

    # [åˆ é™¤è§„åˆ™é€»è¾‘]ï¼šå…³é”®å­—è¿‡æ»¤
    del_keywords = [k.strip() for k in (patch.get('del_rules') or []) if k.strip()]
    if del_keywords:
        final_rules = []
        for rule in config.get('rules', []):
            clean_rule = clean_rule_for_clash(rule)
            # å¦‚æœè§„åˆ™åŒ…å«ä»»ä½•ä¸€ä¸ªåˆ é™¤å…³é”®å­—ï¼Œåˆ™ä¸¢å¼ƒ
            if not any(k in clean_rule for k in del_keywords): 
                final_rules.append(rule)
        config['rules'] = final_rules

    # [æ·»åŠ è§„åˆ™é€»è¾‘]ï¼šå¼ºåˆ¶æ’å…¥åˆ°æœ€å‰
    # ä¿®å¤ï¼šç›´æ¥æ’å…¥ï¼Œä¸è¿‡åº¦æ¸…æ´—ï¼Œä¿ç•™å¤‡æ³¨
    if add_rules_raw:
        for r in reversed(add_rules_raw): 
            if r and r.strip():
                config.setdefault('rules', []).insert(0, r.strip())
             
    return config

@app.on_event("startup")
async def startup_event():
    init_data()
    scheduler.start()
    refresh_scheduler()
    logger.info("Application started, scheduler running.")

@app.get("/api/logs")
async def get_logs(lines: int = 100):
    if not os.path.exists(LOG_FILE):
        return {"logs": []}
    try:
        async with aiofiles.open(LOG_FILE, 'r', encoding='utf-8') as f:
            content = await f.read()
            all_lines = content.splitlines()
            return {"logs": all_lines[-lines:]}
    except Exception as e:
        return {"logs": [f"Error reading logs: {str(e)}"]}

@app.get("/api/data")
async def get_data():
    try:
        if os.path.exists(CONFIG_JSON) and os.path.getsize(CONFIG_JSON) > 0:
            async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
                if 'user_info' not in data:
                    data['user_info'] = UserInfo().dict()
                else:
                    # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
                    default_info = UserInfo().dict()
                    for k, v in default_info.items():
                        if k not in data['user_info']:
                            data['user_info'][k] = v
                return data
        return {}
    except: return {}

@app.post("/api/data")
async def save_data(data: ConfigModel):
    try:
        payload = data.dict(exclude_none=True)
        async with aiofiles.open(CONFIG_JSON, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(payload, indent=2))
        
        refresh_scheduler()
        return {"status": "success"}
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# [æ¸…ç©ºè®¢é˜…æ¥å£]
@app.delete("/api/subscription")
async def delete_subscription():
    try:
        async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)
        
        # æ¸…ç©º
        data['sub_url'] = ""
        data['user_info'] = UserInfo().dict()
        
        async with aiofiles.open(CONFIG_JSON, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2))
            
        # é‡ç½® YAML
        minimal_config = {
            "port": 7890,
            "socks-port": 7891,
            "allow-lan": True,
            "mode": "Rule",
            "log-level": "info",
            "external-controller": "0.0.0.0:9090",
            "proxies": [],
            "proxy-groups": [],
            "rules": []
        }
        async with aiofiles.open(OUTPUT_YAML, 'w', encoding='utf-8') as f:
            await f.write(yaml.dump(minimal_config))
            
        logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºå½“å‰è®¢é˜…åŠé…ç½®")
        return {"status": "success", "msg": "è®¢é˜…å·²æ¸…ç©º"}
    except Exception as e:
        logger.error(f"æ¸…ç©ºè®¢é˜…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/backup")
async def backup_config(include_history: bool = False):
    if not os.path.exists(CONFIG_JSON): raise HTTPException(status_code=404, detail="No config found")
    try:
        async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)
            
        if not include_history:
            # æ¸…é™¤è®¢é˜…æ•æ„Ÿä¿¡æ¯
            data['sub_url'] = ""
            data['sub_history'] = []
            
        temp_path = "/tmp/clashweb_backup.json"
        async with aiofiles.open(temp_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2))
            
        return FileResponse(temp_path, filename="clashweb_backup.json", media_type="application/json")
    except Exception as e:
        raise HTTPException(500, detail=str(e))

@app.post("/api/restore")
async def restore_config(file: UploadFile = File(...)):
    try:
        content = await file.read()
        backup_data = json.loads(content)
        if not isinstance(backup_data, dict): raise ValueError("Format Error")
        
        current_data = {}
        if os.path.exists(CONFIG_JSON):
            with open(CONFIG_JSON, 'r', encoding='utf-8') as f: current_data = json.load(f)
        
        # ä¿æŠ¤é€»è¾‘ï¼šå¦‚æœå¤‡ä»½æ²¡è®¢é˜…ï¼Œä¿ç•™å½“å‰çš„
        if not backup_data.get('sub_url'):
            backup_data['sub_url'] = current_data.get('sub_url', "")
            if not backup_data.get('sub_history'):
                backup_data['sub_history'] = current_data.get('sub_history', [])
        
        merged_data = ConfigModel(**current_data).dict()
        merged_data.update(backup_data)

        async with aiofiles.open(CONFIG_JSON, "w", encoding='utf-8') as f:
            await f.write(json.dumps(merged_data, indent=2))
        
        refresh_scheduler()
        
        summary = {
            "groups": len(merged_data.get('add_groups', [])),
            "rules": len(merged_data.get('add_rules', [])),
            "has_sub": bool(merged_data.get('sub_url'))
        }
        return {"status": "success", "summary": summary}
    except Exception as e:
        logger.error(f"è¿˜åŸå¤±è´¥: {e}")
        raise HTTPException(status_code=400, detail=f"Restore Failed: {str(e)}")

@app.post("/api/restart_containers")
async def restart_containers():
    try:
        async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)
        
        # [ä¿®å¤] å…¼å®¹ä¸­æ–‡é€—å·
        container_str = data.get('restart_containers', '').replace('ï¼Œ', ',')
        targets = [n.strip() for n in container_str.split(',') if n.strip()]

        if not targets: raise HTTPException(400, detail="æœªè®¾ç½®å®¹å™¨")
        
        client = docker.from_env()
        restarted = []
        for name in targets:
            try:
                client.containers.get(name).restart()
                restarted.append(name)
                logger.info(f"æ‰‹åŠ¨è§¦å‘ - å®¹å™¨å·²é‡å¯: {name}")
            except Exception as e:
                logger.error(f"æ‰‹åŠ¨è§¦å‘ - é‡å¯å¤±è´¥ {name}: {e}")
            
        if not restarted:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æœ‰æ•ˆå®¹å™¨")
            
        return {"status": "success", "msg": f"å·²é‡å¯: {', '.join(restarted)}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Docker Error: {str(e)}")

@app.post("/api/download")
async def download_config(req: DownloadRequest):
    if not req.url: raise HTTPException(status_code=400, detail="Missing URL")

    try:
        async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)
    except: data = {}
    
    existing_history_entry = next((h for h in data.get('sub_history', []) if h.get('url') == req.url), None)
    data['sub_url'] = req.url
    
    try:
        # è·å–æµé‡ & ä¸‹è½½
        fetched_info = await internal_process_subscription(req.url, data)
        
        # å†å²è®°å½•é€»è¾‘
        airport_name = existing_history_entry.get("name", "æœªçŸ¥æœºåœº") if existing_history_entry else "æœªçŸ¥æœºåœº"
        traffic_info = existing_history_entry.get("info", {}) if existing_history_entry else {}
        
        if fetched_info:
            airport_name = fetched_info.get("name", "æœªçŸ¥æœºåœº")
            traffic_info = {
                "upload": fetched_info.get("upload", 0),
                "download": fetched_info.get("download", 0),
                "total": fetched_info.get("total", 0),
                "expire": fetched_info.get("expire", 0)
            }
        
        history = data.get('sub_history', [])
        history = [h for h in history if h.get('url') != req.url]
        
        new_record = {
            "url": req.url,
            "date": datetime.now().strftime('%Y-%m-%d %H:%M'),
            "name": airport_name,
            "info": traffic_info
        }
        history.insert(0, new_record)
        
        if len(history) > 10: history = history[:10]
        data['sub_history'] = history
        
        async with aiofiles.open(CONFIG_JSON, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2))
            
    except Exception as e:
        logger.error(f"å¤„ç†è®¢é˜…å‡ºé”™: {e}")
        # å‡ºé”™ä¹Ÿè¦ä¿å­˜ URL
        async with aiofiles.open(CONFIG_JSON, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2))
        raise HTTPException(status_code=500, detail=f"Processing Error: {str(e)}")
        
    return {"status": "success"}

@app.get("/api/analysis")
async def analyze_config():
    """
    åˆ†æé…ç½®ï¼Œå¹¶æ ‡è®°æ¥æº
    """
    if not os.path.exists(OUTPUT_YAML) or os.path.getsize(OUTPUT_YAML) == 0:
        return {"status": "empty", "groups": [], "rules": [], "rule_count": 0, "regions": []}
    
    try:
        async with aiofiles.open(OUTPUT_YAML, 'r', encoding='utf-8') as f:
            content = await f.read()
            config = yaml.safe_load(content)
            if not config: return {"status": "empty"}
        
        user_config = {}
        try:
            if os.path.exists(CONFIG_JSON):
                async with aiofiles.open(CONFIG_JSON, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    user_config = json.loads(content)
        except: pass

        custom_group_names = set()
        for g in user_config.get('add_groups', []):
            if g.get('name'):
                custom_group_names.add(g['name'])
        
        custom_rules_map = {} 
        for r in user_config.get('add_rules', []):
            clean = clean_rule_for_clash(r)
            custom_rules_map[clean] = r

        rule_usage = Counter()
        final_display_rules = []
        
        for r in config.get('rules', []):
            target = get_rule_target(r)
            if target: rule_usage[target] += 1
            
            clean_r = clean_rule_for_clash(r)
            is_custom = clean_r in custom_rules_map
            display_str = custom_rules_map[clean_r] if is_custom else r
            
            final_display_rules.append({
                "str": display_str,
                "source": "custom" if is_custom else "native"
            })

        groups_info = []
        for g in config.get('proxy-groups', []):
            g_name = g['name']
            source = "custom" if g_name in custom_group_names else "native"
            groups_info.append({
                "name": g_name,
                "type": g.get('type', 'select'),
                "rule_count": rule_usage.get(g_name, 0),
                "source": source 
            })
        
        proxies = config.get('proxies', [])
        region_map = {
            "hk": "é¦™æ¸¯", "hong": "é¦™æ¸¯", "é¦™æ¸¯": "é¦™æ¸¯",
            "tw": "å°æ¹¾", "tai": "å°æ¹¾", "å°æ¹¾": "å°æ¹¾",
            "jp": "æ—¥æœ¬", "japan": "æ—¥æœ¬", "æ—¥æœ¬": "æ—¥æœ¬",
            "us": "ç¾å›½", "america": "ç¾å›½", "united": "ç¾å›½", "ç¾å›½": "ç¾å›½",
            "sg": "æ–°åŠ å¡", "sing": "æ–°åŠ å¡", "æ–°åŠ å¡": "æ–°åŠ å¡",
            "kr": "éŸ©å›½", "korea": "éŸ©å›½", "éŸ©å›½": "éŸ©å›½",
            "uk": "è‹±å›½", "gb": "è‹±å›½", "è‹±å›½": "è‹±å›½",
            "de": "å¾·å›½", "ger": "å¾·å›½", "å¾·å›½": "å¾·å›½",
            "ca": "åŠ æ‹¿å¤§", "can": "åŠ æ‹¿å¤§", "åŠ æ‹¿å¤§": "åŠ æ‹¿å¤§",
            "tr": "åœŸè€³å…¶", "tur": "åœŸè€³å…¶", "åœŸ": "åœŸè€³å…¶",
            "fr": "æ³•å›½", "france": "æ³•å›½", "æ³•": "æ³•å›½",
            "ru": "ä¿„ç½—æ–¯", "russia": "ä¿„ç½—æ–¯", "ä¿„": "ä¿„ç½—æ–¯"
        }
        icons = {
            "é¦™æ¸¯": "ğŸ‡­ğŸ‡°", "å°æ¹¾": "ğŸ‡¹ğŸ‡¼", "æ—¥æœ¬": "ğŸ‡¯ğŸ‡µ", "ç¾å›½": "ğŸ‡ºğŸ‡¸", 
            "æ–°åŠ å¡": "ğŸ‡¸ğŸ‡¬", "éŸ©å›½": "ğŸ‡°ğŸ‡·", "è‹±å›½": "ğŸ‡¬ğŸ‡§", "å¾·å›½": "ğŸ‡©ğŸ‡ª", 
            "åŠ æ‹¿å¤§": "ğŸ‡¨ğŸ‡¦", "åœŸè€³å…¶": "ğŸ‡¹ğŸ‡·", "æ³•å›½": "ğŸ‡«ğŸ‡·", "ä¿„ç½—æ–¯": "ğŸ‡·ğŸ‡º", "å…¶ä»–": "ğŸŒ"
        }
        
        counts = {}
        for p in proxies:
            name = p.get('name', '').lower()
            found = False
            for k, v in region_map.items():
                if k in name:
                    if v not in counts: counts[v] = {"name": v, "icon": icons.get(v, "ğŸŒ"), "count": 0}
                    counts[v]['count'] += 1
                    found = True
                    break
            if not found:
                if "å…¶ä»–" not in counts: counts["å…¶ä»–"] = {"name": "å…¶ä»–", "icon": "ğŸŒ", "count": 0}
                counts["å…¶ä»–"]['count'] += 1
        
        regions = sorted(counts.values(), key=lambda x: x['count'], reverse=True)
        final_regions = [r for r in regions if r['name'] != 'å…¶ä»–']
        if "å…¶ä»–" in counts: final_regions.append(counts["å…¶ä»–"])

        mtime = os.path.getmtime(OUTPUT_YAML)
        ts_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            "status": "success", 
            "groups": groups_info, 
            "rules": final_display_rules, 
            "rule_count": len(final_display_rules), 
            "regions": final_regions, 
            "total_nodes": len(proxies), 
            "update_time": ts_str,
            "ts": datetime.now().timestamp()
        }
    except Exception as e: return {"status": "error", "msg": str(e)}

# --- é™æ€æ–‡ä»¶æŒ‚è½½ ---
if os.path.exists("images"):
    app.mount("/images", StaticFiles(directory="images"), name="images")

app.mount("/", StaticFiles(directory="static", html=True), name="static")